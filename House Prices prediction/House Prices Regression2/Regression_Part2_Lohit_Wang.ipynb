{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression: HOUSE PRICES PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract: The project aims to predict the final price of each home with around 80 features regarding every aspect of residential homes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is from the Kaggle website. https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data There are two datasets in total. The training dataset has 1460 rows and 81 columns, including the target variable. The testing dataset has 1459 rows and 80 columns. The detailed description is shown in the file 'data_description.txt.' In this project, we will train the model on the training dataset using the best model and predict the target values for the testing dataset.\n",
    "\n",
    "Here is a brief description of the features of the dataset: \n",
    "\n",
    "1. SalePrice: the property's sale price in dollars. (Target variable)\n",
    "2. MSSubClass: The building class\n",
    "3. MSZoning: The general zoning classification\n",
    "4. LotFrontage: Linear feet of street connected to property\n",
    "5. LotArea: Lot size in square feet\n",
    "6. Street: Type of road access\n",
    "7. Alley: Type of alley access\n",
    "8. LotShape: General shape of property\n",
    "9. LandContour: Flatness of the property\n",
    "10. Utilities: Type of utilities available\n",
    "11. LotConfig: Lot configuration\n",
    "12. LandSlope: Slope of property\n",
    "13. Neighborhood: Physical locations within Ames city limits\n",
    "14. Condition1: Proximity to main road or railroad\n",
    "15. Condition2: Proximity to main road or railroad (if a second is present)\n",
    "16. BldgType: Type of dwelling\n",
    "17. HouseStyle: Style of dwelling\n",
    "18. OverallQual: Overall material and finish quality\n",
    "19. OverallCond: Overall condition rating\n",
    "20. YearBuilt: Original construction date\n",
    "21. YearRemodAdd: Remodel date\n",
    "22. RoofStyle: Type of roof\n",
    "23. RooMatl: Roof material\n",
    "24. Exterior1st: Exterior covering on house\n",
    "25. Exterior2nd: Exterior covering on house (if more than one material)\n",
    "26. MasVnrType: Masonry veneer type\n",
    "27. MasVnrArea: Masonry veneer area in square feet\n",
    "28. ExterQual: Exterior material quality\n",
    "29. ExterCond: Present condition of the material on the exterior\n",
    "30. Foundation: Type of foundation\n",
    "31. BsmtQual: Height of the basement\n",
    "32. BsmtCond: General condition of the basement\n",
    "33. BsmtExposure: Walkout or garden level basement walls\n",
    "34. BsmtFinType1: Quality of basement finished area\n",
    "35. BsmtFinSF1: Type 1 finished square feet\n",
    "36. BsmtFinType2: Quality of second finished area (if present)\n",
    "37. BsmtFinSF2: Type 2 finished square feet\n",
    "38. BsmtUnfSF: Unfinished square feet of basement area\n",
    "39. TotalBsmtSF: Total square feet of basement area\n",
    "40. Heating: Type of heating\n",
    "41. HeatingQC: Heating quality and condition\n",
    "42. CentralAir: Central air conditioning\n",
    "43. Electrical: Electrical system\n",
    "44. 1stFlrSF: First Floor square feet\n",
    "45. 2ndFlrSF: Second floor square feet\n",
    "46. LowQualFinSF: Low quality finished square feet (all floors)\n",
    "47. GrLivArea: Above grade (ground) living area square feet\n",
    "48. BsmtFullBath: Basement full bathrooms\n",
    "49. BsmtHalfBath: Basement half bathrooms\n",
    "50. FullBath: Full bathrooms above grade\n",
    "51. HalfBath: Half baths above grade\n",
    "52. Bedroom: Number of bedrooms above basement level\n",
    "53. Kitchen: Number of kitchens\n",
    "54. KitchenQual: Kitchen quality\n",
    "55. TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n",
    "56. Functional: Home functionality rating\n",
    "57. Fireplaces: Number of fireplaces\n",
    "58. FireplaceQu: Fireplace quality\n",
    "59. GarageType: Garage location\n",
    "60. GarageYrBlt: Year garage was built\n",
    "61. GarageFinish: Interior finish of the garage\n",
    "62. GarageCars: Size of garage in car capacity\n",
    "63. GarageArea: Size of garage in square feet\n",
    "64. GarageQual: Garage quality\n",
    "65. GarageCond: Garage condition\n",
    "66. PavedDrive: Paved driveway\n",
    "67. WoodDeckSF: Wood deck area in square feet\n",
    "68. OpenPorchSF: Open porch area in square feet\n",
    "69. EnclosedPorch: Enclosed porch area in square feet\n",
    "70. 3SsnPorch: Three season porch area in square feet\n",
    "71. ScreenPorch: Screen porch area in square feet\n",
    "72. PoolArea: Pool area in square feet\n",
    "73. PoolQC: Pool quality\n",
    "74. Fence: Fence quality\n",
    "75. MiscFeature: Miscellaneous feature not covered in other categories\n",
    "76. MiscVal: Value of miscellaneous feature\n",
    "77. MoSold: Month Sold\n",
    "78. YrSold: Year Sold\n",
    "79. SaleType: Type of sale\n",
    "80. SaleCondition: Condition of sale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use the same datasets and the preprocessing steps as Project 1, we load the datasets that have been cleansed and transformed in Project 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_forPred = pd.read_csv('houseprice_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 106)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 105)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_df.drop('SalePrice', axis = 1)\n",
    "y = train_df.SalePrice\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set: (1094, 105)   Size of test set: (365, 105)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of train set: {}   Size of test set: {}\".format(X_train_org.shape, X_test_org.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we split the data, we get the training set of 1094 and the testing set of 365."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test metrices -> mean: 0.2707016845791734, std: 0.14571551732041682\n",
      "y_train metrices -> mean: 0.20420521525169033, std: 0.11221269827058238\n"
     ]
    }
   ],
   "source": [
    "y_test = (y_test - y_test.min()) / (y_test.max() - y_test.min())\n",
    "y_train = (y_train - y_train.min()) / (y_train.max() - y_train.min())\n",
    "print(f'y_test metrices -> mean: {y_test.mean()}, std: {y_test.std()}')\n",
    "print(f'y_train metrices -> mean: {y_train.mean()}, std: {y_train.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is not a normal distribution, we use MinMaxScaler instead of StandardScaler to normalize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use grid search to find the best hyper-parameters for each model. Additionally, this is regression analysis, so we use ``R2`` as our scoring metrics to find the optimal model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the Ridge model and the Lasso model with bagging and pasting methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Ridge with Bagging and Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters(Ridge Bagging): {'max_samples': 0.1, 'n_estimators': 100}\n",
      "Best parameters(Ridge Pasting): {'max_samples': 0.1, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# define the best ridge model\n",
    "best_ridge = Ridge(alpha=0.1,\n",
    "                   normalize=True,\n",
    "                   tol=1e-06\n",
    "                  )\n",
    "\n",
    "# define the best ridge model with bagging\n",
    "bag_ridge = BaggingRegressor(best_ridge,\n",
    "                           bootstrap=True,\n",
    "                           n_jobs=-1, \n",
    "                           random_state=0,\n",
    "                           oob_score = True)\n",
    "\n",
    "# set param_grid\n",
    "param_grid = {'n_estimators':[10, 50, 100],\n",
    "              'max_samples':[0.1, 1.0, 10]\n",
    "             }\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_ridge_bag = GridSearchCV(bag_ridge, param_grid, \n",
    "                         cv=cv,\n",
    "                         scoring=scoring)\n",
    "\n",
    "# fit the grid search\n",
    "gscv_ridge_bag.fit(X_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters(Ridge Bagging): {}\".format(gscv_ridge_bag.best_params_))\n",
    "\n",
    "# define the best ridge model with pasting\n",
    "pas_ridge = BaggingRegressor(best_ridge,\n",
    "                           bootstrap=False,\n",
    "                           n_jobs=-1, \n",
    "                           random_state=0)\n",
    "\n",
    "# set param_grid\n",
    "param_grid = {'n_estimators':[10, 50, 100],\n",
    "              'max_samples':[0.1, 1.0, 10]\n",
    "             }\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_ridge_pas = GridSearchCV(pas_ridge, param_grid, \n",
    "                         cv=cv,\n",
    "                         scoring=scoring,\n",
    "                         return_train_score=True)\n",
    "\n",
    "# fit the grid search\n",
    "gscv_ridge_pas.fit(X_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters(Ridge Pasting): {}\".format(gscv_ridge_pas.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the best parameters: {'alpha': 0.1, 'normalize': True, 'tol': 1e-06} that we gain from Project 1 for Ridge Regressor. The result above shows the best parameters for Bagging Regressor and Pasting Regressor with grid search method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Bagging\n",
      "Train score: 0.79\n",
      "Test score: 0.51\n",
      "Out-of-Bag score: 0.77\n",
      "\n",
      "Ridge Pasting\n",
      "Train score: 0.74\n",
      "Test score: 0.43\n"
     ]
    }
   ],
   "source": [
    "# define the best ridge model with best bagging\n",
    "best_ridge_bag = BaggingRegressor(\n",
    "                            gscv_ridge_bag,\n",
    "                            max_samples=gscv_ridge_bag.best_params_['max_samples'],\n",
    "                            n_estimators=gscv_ridge_bag.best_params_['n_estimators'],\n",
    "                            bootstrap=True,\n",
    "                            n_jobs=-1, \n",
    "                            random_state=0,\n",
    "                            oob_score = True)\n",
    "\n",
    "# define the best ridge model with best pasting\n",
    "best_ridge_pas = BaggingRegressor(\n",
    "                            gscv_ridge_pas,\n",
    "                            max_samples=gscv_ridge_pas.best_params_['max_samples'],\n",
    "                            n_estimators=gscv_ridge_pas.best_params_['n_estimators'],\n",
    "                            bootstrap=False,\n",
    "                            n_jobs=-1, \n",
    "                            random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "best_ridge_bag.fit(X_train, y_train)\n",
    "best_ridge_pas.fit(X_train, y_train)\n",
    "\n",
    "# Bagging result\n",
    "print('Ridge Bagging')\n",
    "print('Train score: {:.2f}'.format(best_ridge_bag.score(X_train, y_train)))\n",
    "print('Test score: {:.2f}'.format(best_ridge_bag.score(X_test, y_test)))\n",
    "print('Out-of-Bag score: {:.2f}'.format(best_ridge_bag.oob_score_))\n",
    "\n",
    "# Pasting result\n",
    "print('\\nRidge Pasting')\n",
    "print('Train score: {:.2f}'.format(best_ridge_pas.score(X_train, y_train)))\n",
    "print('Test score: {:.2f}'.format(best_ridge_pas.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the Ridge model with bagging and pasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Bagging\n",
      "R2_ridge_bag: 0.51\n",
      "RMSE_ridge_bag: 0.10\n",
      "\n",
      "Ridge Pasting\n",
      "R2_ridge_pas: 0.43\n",
      "RMSE_ridge_pas: 0.11\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_ridge_bag = best_ridge_bag.predict(X_test)\n",
    "y_pred_ridge_pas = best_ridge_pas.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Ridge Bagging')\n",
    "R2_ridge_bag = r2_score(y_test, y_pred_ridge_bag)\n",
    "print(\"R2_ridge_bag: {:.2f}\".format(R2_ridge_bag)) \n",
    "RMSE_ridge_bag  = np.sqrt(metrics.mean_squared_error(y_test, y_pred_ridge_bag))\n",
    "print('RMSE_ridge_bag: {:.2f}'.format(RMSE_ridge_bag))\n",
    "\n",
    "print('\\nRidge Pasting')\n",
    "R2_ridge_pas = r2_score(y_test, y_pred_ridge_pas)\n",
    "print(\"R2_ridge_pas: {:.2f}\".format(R2_ridge_pas)) \n",
    "RMSE_ridge_pas = np.sqrt(metrics.mean_squared_error(y_test, y_pred_ridge_pas))\n",
    "print('RMSE_ridge_pas: {:.2f}'.format(RMSE_ridge_pas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge (Project1)\n",
      "R2_ridge: 0.54\n",
      "RMSE_ridge: 0.10\n"
     ]
    }
   ],
   "source": [
    "print('Ridge (Project1)')\n",
    "print(\"R2_ridge: {:.2f}\".format(0.54)) \n",
    "print('RMSE_ridge: {:.2f}'.format(0.10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above indicate the R2 score and the RMSE value of the original Ridge model and the ones with bagging and pasting methods. Therefore, we can infer that using the bagging and pasting methods does not improve the Ridge model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Lasso with Bagging and Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters(Lasso Bagging): {'max_samples': 1.0, 'n_estimators': 10}\n",
      "Best parameters(Lasso Pasting): {'max_samples': 1.0, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# define the best lasso model\n",
    "best_lasso = Lasso(alpha=0.001)\n",
    "\n",
    "# define the best lasso model with bagging\n",
    "bag_lasso = BaggingRegressor(best_lasso,\n",
    "                           bootstrap=True,\n",
    "                           n_jobs=-1, \n",
    "                           random_state=0,\n",
    "                           oob_score = True)\n",
    "\n",
    "# set param_grid\n",
    "param_grid = {'n_estimators':[10, 50, 100],\n",
    "              'max_samples':[0.1, 1.0, 10]\n",
    "             }\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_lasso_bag = GridSearchCV(bag_lasso, param_grid, \n",
    "                         cv=cv,\n",
    "                         scoring=scoring,\n",
    "                         return_train_score=True)\n",
    "\n",
    "# fit the grid search\n",
    "gscv_lasso_bag.fit(X_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters(Lasso Bagging): {}\".format(gscv_lasso_bag.best_params_))\n",
    "\n",
    "# define the best lasso model with pasting\n",
    "pas_lasso = BaggingRegressor(best_lasso,\n",
    "                           bootstrap=False,\n",
    "                           n_jobs=-1, \n",
    "                           random_state=0)\n",
    "\n",
    "# set param_grid\n",
    "param_grid = {'n_estimators':[10, 50, 100],\n",
    "              'max_samples':[0.1, 1.0, 10]\n",
    "             }\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_lasso_pas = GridSearchCV(pas_lasso, param_grid, \n",
    "                         cv=cv,\n",
    "                         scoring=scoring,\n",
    "                         return_train_score=True)\n",
    "# fit the grid search\n",
    "gscv_lasso_pas.fit(X_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters(Lasso Pasting): {}\".format(gscv_lasso_pas.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the best parameters: {'alpha': 0.001} that we gain from Project 1 for Lasso Regressor. The result above shows the best parameters for Bagging Regressor and Pasting Regressor with grid search method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Bagging\n",
      "Train score: 0.81\n",
      "Test score: 0.52\n",
      "Out-of-Bag score: 0.76\n",
      "\n",
      "Lasso Pasting\n",
      "Train score: 0.80\n",
      "Test score: 0.50\n"
     ]
    }
   ],
   "source": [
    "# define the best lasso model with best bagging\n",
    "best_lasso_bag = BaggingRegressor(\n",
    "                            gscv_lasso_bag,\n",
    "                            max_samples=gscv_lasso_bag.best_params_['max_samples'],\n",
    "                            n_estimators=gscv_lasso_bag.best_params_['n_estimators'],\n",
    "                            bootstrap=True,\n",
    "                            n_jobs=-1, \n",
    "                            random_state=0,\n",
    "                            oob_score = True)\n",
    "\n",
    "# define the best lasso model with best pasting\n",
    "best_lasso_pas = BaggingRegressor(\n",
    "                            gscv_lasso_pas,\n",
    "                            max_samples=gscv_lasso_pas.best_params_['max_samples'],\n",
    "                            n_estimators=gscv_lasso_pas.best_params_['n_estimators'],\n",
    "                            bootstrap=False,\n",
    "                            n_jobs=-1, \n",
    "                            random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "best_lasso_bag.fit(X_train, y_train)\n",
    "best_lasso_pas.fit(X_train, y_train)\n",
    "\n",
    "# Bagging result\n",
    "print('Lasso Bagging')\n",
    "print('Train score: {:.2f}'.format(best_lasso_bag.score(X_train, y_train)))\n",
    "print('Test score: {:.2f}'.format(best_lasso_bag.score(X_test, y_test)))\n",
    "print('Out-of-Bag score: {:.2f}'.format(best_lasso_bag.oob_score_))\n",
    "\n",
    "\n",
    "# Pasting result\n",
    "print('\\nLasso Pasting')\n",
    "print('Train score: {:.2f}'.format(best_lasso_pas.score(X_train, y_train)))\n",
    "print('Test score: {:.2f}'.format(best_lasso_pas.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the Lasso model with bagging and pasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Bagging\n",
      "R2_lasso_bag: 0.52\n",
      "RMSE_lasso_bag: 0.10\n",
      "\n",
      "Lasso Pasting\n",
      "R2_lasso_pas: 0.50\n",
      "RMSE_lasso_pas: 0.10\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_lasso_bag = best_lasso_bag.predict(X_test)\n",
    "y_pred_lasso_pas = best_lasso_pas.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Lasso Bagging')\n",
    "R2_lasso_bag = r2_score(y_test, y_pred_lasso_bag)\n",
    "print(\"R2_lasso_bag: {:.2f}\".format(R2_lasso_bag)) \n",
    "RMSE_lasso_bag  = np.sqrt(metrics.mean_squared_error(y_test, y_pred_lasso_bag))\n",
    "print('RMSE_lasso_bag: {:.2f}'.format(RMSE_lasso_bag))\n",
    "\n",
    "print('\\nLasso Pasting')\n",
    "R2_lasso_pas = r2_score(y_test, y_pred_lasso_pas)\n",
    "print(\"R2_lasso_pas: {:.2f}\".format(R2_lasso_pas)) \n",
    "RMSE_lasso_pas = np.sqrt(metrics.mean_squared_error(y_test, y_pred_lasso_pas))\n",
    "print('RMSE_lasso_pas: {:.2f}'.format(RMSE_lasso_pas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso (Project1)\n",
      "R2_lasso: 0.51\n",
      "RMSE_lasso: 0.11\n"
     ]
    }
   ],
   "source": [
    "print('Lasso (Project1)')\n",
    "print(\"R2_lasso: {:.2f}\".format(0.51)) \n",
    "print('RMSE_lasso: {:.2f}'.format(0.11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above indicate the R2 score and the RMSE value of the original Lasso model and the ones with bagging and pasting methods. Therefore, we can infer that using applying bagging methods can improve the R2 score of the Lasso model, but it may slightly increase the RMSE value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the KNN Regressor model and the Linear Regression model with Adaboost boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  KNN Regressor with AdaBoost boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters(KNN AdaBoost): {'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# define the best knn model\n",
    "best_knn = KNeighborsRegressor(\n",
    "    n_neighbors=8)\n",
    "\n",
    "# define the best knn model with AdaBoost\n",
    "ada_knn = AdaBoostRegressor(best_knn, \n",
    "                           learning_rate=1.0, \n",
    "                           random_state=0)\n",
    "\n",
    "# set param_grid\n",
    "param_grid = {'n_estimators':[10,50,100]}\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_knn_ada = GridSearchCV(ada_knn, param_grid, \n",
    "                         cv=cv,\n",
    "                         scoring=scoring,\n",
    "                         return_train_score=True)\n",
    "\n",
    "# fit the grid search\n",
    "gscv_knn_ada.fit(X_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters(KNN AdaBoost): {}\".format(gscv_knn_ada.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the best parameters: {'n_neighbors': 8} that we gain from Project 1 for KNN Regressor. The result above shows the best parameters for AdaBoost Regressor with grid search method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN AdaBoost\n",
      "Train score: 0.88\n",
      "Test score: 0.41\n"
     ]
    }
   ],
   "source": [
    "# define the best knn model with best AdaBoost\n",
    "best_knn_ada = AdaBoostRegressor(\n",
    "                            gscv_knn_ada,\n",
    "                            n_estimators=gscv_knn_ada.best_params_['n_estimators'],\n",
    "                            learning_rate=1.0, \n",
    "                            random_state=0)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "best_knn_ada.fit(X_train, y_train)\n",
    "\n",
    "# AdaBoost result\n",
    "print('KNN AdaBoost')\n",
    "print('Train score: {:.2f}'.format(best_knn_ada.score(X_train, y_train)))\n",
    "print('Test score: {:.2f}'.format(best_knn_ada.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the KNN model with AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear AdaBoost\n",
      "R2_linear_ada: 0.41\n",
      "RMSE_knn_ada: 0.11\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_knn_ada = best_knn_ada.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Linear AdaBoost')\n",
    "R2_knn_ada = r2_score(y_test, y_pred_knn_ada)\n",
    "print(\"R2_linear_ada: {:.2f}\".format(R2_knn_ada)) \n",
    "RMSE_knn_ada  = np.sqrt(metrics.mean_squared_error(y_test, y_pred_knn_ada))\n",
    "print('RMSE_knn_ada: {:.2f}'.format(RMSE_knn_ada))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (Project1)\n",
      "R2_knn: 0.29\n",
      "RMSE_knn: 0.12\n"
     ]
    }
   ],
   "source": [
    "print('KNN (Project1)')\n",
    "print(\"R2_knn: {:.2f}\".format(0.29)) \n",
    "print('RMSE_knn: {:.2f}'.format(0.12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above indicate the R2 score and the RMSE value of the original KNN model and the one with the Adaboost boosting method. Since the R2 score increases from 0.29 to 0.41, and the RMSE value decreases from 0.12 to 0.11. Hence, we can infer that the KNN model with the Adaboost boosting method is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.  Linear Regression with AdaBoost boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters(Linear AdaBoost): {'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# define the best linear model\n",
    "best_linear = LinearRegression(fit_intercept=True, \n",
    "                               normalize=True)\n",
    "\n",
    "# define the linear model with AdaBoost\n",
    "ada_linear = AdaBoostRegressor(best_linear, \n",
    "                           learning_rate=1.0, \n",
    "                           random_state=0)\n",
    "\n",
    "# set param_grid\n",
    "param_grid = {'n_estimators':[10,50,100]}\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_linear_ada = GridSearchCV(ada_linear, param_grid, \n",
    "                         cv=cv,\n",
    "                         scoring=scoring,\n",
    "                         return_train_score=True)\n",
    "\n",
    "# fit the grid search\n",
    "gscv_linear_ada.fit(X_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters(Linear AdaBoost): {}\".format(gscv_linear_ada.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the best parameters: {'fit_intercept': True, 'normalize': True} that we gain from Project 1 for Linear Regressor. The result above shows the best parameters for AdaBoost Regressor with grid search method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear AdaBoost\n",
      "Train score: 0.88\n",
      "Test score: 0.61\n"
     ]
    }
   ],
   "source": [
    "# define the best linear model with best AdaBoost\n",
    "best_linear_ada = AdaBoostRegressor(\n",
    "                            gscv_linear_ada,\n",
    "                            n_estimators=gscv_linear_ada.best_params_['n_estimators'],\n",
    "                            learning_rate=1.0, \n",
    "                            random_state=0)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "best_linear_ada.fit(X_train, y_train)\n",
    "\n",
    "# AdaBoost result\n",
    "print('Linear AdaBoost')\n",
    "print('Train score: {:.2f}'.format(best_linear_ada.score(X_train, y_train)))\n",
    "print('Test score: {:.2f}'.format(best_linear_ada.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the Linear Regressor model with Adaboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear AdaBoost\n",
      "R2_linear_ada: 0.61\n",
      "RMSE_linear_ada: 0.09\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_linear_ada = best_linear_ada.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Linear AdaBoost')\n",
    "R2_linear_ada = r2_score(y_test, y_pred_linear_ada)\n",
    "print(\"R2_linear_ada: {:.2f}\".format(R2_linear_ada)) \n",
    "RMSE_linear_ada  = np.sqrt(metrics.mean_squared_error(y_test, y_pred_linear_ada))\n",
    "print('RMSE_linear_ada: {:.2f}'.format(RMSE_linear_ada))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear (Project1)\n",
      "R2_linear: 0.57\n",
      "RMSE_linear: 0.10\n"
     ]
    }
   ],
   "source": [
    "print('Linear (Project1)')\n",
    "print(\"R2_linear: {:.2f}\".format(0.57)) \n",
    "print('RMSE_linear: {:.2f}'.format(0.10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above indicate the R2 score and the RMSE value of the original Linear model and the one with the Adaboost boosting method. Since the R2 score increases from 0.57 to 0.61, and the RMSE value decreases from 0.10 to 0.09. Hence, we can infer that the Linear model with the Adaboost boosting method is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters(GradientBoosting): {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# define the Gradient boosting model\n",
    "grad_reg = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "# set param_grid\n",
    "param_grid = {'max_depth':[1,2,5],\n",
    "              'n_estimators':[1, 10, 100],\n",
    "              'learning_rate':[0.1, 0.5, 1.0]}\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_grad_reg = GridSearchCV(grad_reg, param_grid, \n",
    "                         cv=cv,\n",
    "                         scoring=scoring,\n",
    "                         return_train_score=True)\n",
    "\n",
    "# fit the grid search\n",
    "gscv_grad_reg.fit(X_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters(GradientBoosting): {}\".format(gscv_grad_reg.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boosting\n",
      "Train score: 0.94\n",
      "Test score: 0.56\n"
     ]
    }
   ],
   "source": [
    "# define the best Gradient boosting model \n",
    "best_grad_reg = GradientBoostingRegressor(\n",
    "                           max_depth=gscv_grad_reg.best_params_['max_depth'],\n",
    "                           n_estimators=gscv_grad_reg.best_params_['n_estimators'],\n",
    "                           learning_rate=gscv_grad_reg.best_params_['learning_rate'], \n",
    "                           random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "best_grad_reg.fit(X_train, y_train)\n",
    "\n",
    "# Gradient result\n",
    "print('Gradient boosting')\n",
    "print('Train score: {:.2f}'.format(best_grad_reg.score(X_train, y_train)))\n",
    "print('Test score: {:.2f}'.format(best_grad_reg.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the Gradient boosting model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boosting\n",
      "R2_grad_reg: 0.56\n",
      "RMSE_grad_reg: 0.10\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_grad_reg = best_grad_reg.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('Gradient boosting')\n",
    "R2_grad_reg = r2_score(y_test, y_pred_grad_reg)\n",
    "print(\"R2_grad_reg: {:.2f}\".format(R2_grad_reg)) \n",
    "RMSE_grad_reg  = np.sqrt(metrics.mean_squared_error(y_test, y_pred_grad_reg))\n",
    "print('RMSE_grad_reg: {:.2f}'.format(RMSE_grad_reg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the R2 score and the RMSE value of the Gradient boosting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (1094, 105)\n",
      "Reduced shape: (1094, 48)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 0.95)\n",
    "X_reduced_train = pca.fit_transform(X_train)\n",
    "X_reduced_test = pca.transform(X_test)\n",
    "# pca.n_components_\n",
    "\n",
    "print('Original shape: {}'.format(X_train.shape))\n",
    "print('Reduced shape: {}'.format(X_reduced_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preserve 95% of the variability in the data, we set n_components=0.95. After we apply PCA to reduce dimension, the number of features reduces from 105 to 48."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. PCA - KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters for PCA KNN Regressor: {'n_neighbors': 8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Set param_grid\n",
    "param_grid = {'n_neighbors': np.arange(1, 10, 1)}\n",
    "\n",
    "# Define the model\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# Use GridSearch\n",
    "gscv_knn = GridSearchCV(knn, \n",
    "                        param_grid, \n",
    "                        cv=cv,\n",
    "                        scoring=scoring,\n",
    "                        return_train_score=True\n",
    "                       )\n",
    "\n",
    "# Fit the model\n",
    "gscv_knn.fit(X_reduced_train, y_train)\n",
    "\n",
    "#results = pd.DataFrame(gscv_knn.cv_results_)\n",
    "\n",
    "#bestParamsRow = results.sort_values(\n",
    "#    ['rank_test_score'])[results['mean_train_score']!= 1].iloc[0]['params']\n",
    "\n",
    "print(\"Best hyper-parameters for PCA KNN Regressor: {}\".format(gscv_knn.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA KNN\n",
      "Train score: 0.69\n",
      "Test score: 0.30\n"
     ]
    }
   ],
   "source": [
    "# Set the best KNN\n",
    "pca_knn = KNeighborsRegressor(\n",
    "    n_neighbors=gscv_knn.best_params_['n_neighbors'])\n",
    "\n",
    "# fit the model\n",
    "pca_knn.fit(X_reduced_train, y_train)\n",
    "\n",
    "# train and test score\n",
    "print('PCA KNN')\n",
    "print(\"Train score: {:.2f}\".format(pca_knn.score(X_reduced_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(pca_knn.score(X_reduced_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the PCA KNN Regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA KNN\n",
      "R2_pca_knn: 0.30\n",
      "RMSE_pca_knn: 0.12\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_pca_knn = pca_knn.predict(X_reduced_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('PCA KNN')\n",
    "R2_pca_knn = r2_score(y_test, y_pred_pca_knn)\n",
    "print(\"R2_pca_knn: {:.2f}\".format(R2_pca_knn)) \n",
    "RMSE_pca_knn = np.sqrt(metrics.mean_squared_error(y_test, y_pred_pca_knn))\n",
    "print('RMSE_pca_knn: {:.2f}'.format(RMSE_pca_knn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the R2 score and the RMSE value of the PCA KNN Regressor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. PCA - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best parameters for PCA Linear Regression: {'fit_intercept': True, 'normalize': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# set param_grid\n",
    "param_grid = {'fit_intercept':[True,False], \n",
    "              'normalize':[True,False]}\n",
    "\n",
    "# define the model\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# Use GridSearch\n",
    "gscv_linreg = GridSearchCV(linreg, param_grid,\n",
    "                           cv=cv,\n",
    "                           scoring=scoring,\n",
    "                           verbose=1, \n",
    "                           return_train_score=True\n",
    "                          )\n",
    "\n",
    "# Fit the model\n",
    "gscv_linreg.fit(X_reduced_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters for PCA Linear Regression: {}\".format(gscv_linreg.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Linear\n",
      "Train score: 0.79\n",
      "Test score: 0.51\n"
     ]
    }
   ],
   "source": [
    "# set best linear\n",
    "pca_linreg = LinearRegression(\n",
    "    normalize=gscv_linreg.best_params_['normalize'],\n",
    "    fit_intercept=gscv_linreg.best_params_['fit_intercept'])\n",
    "\n",
    "# Fit the model\n",
    "pca_linreg.fit(X_reduced_train, y_train)\n",
    "\n",
    "# train and test score\n",
    "print('PCA Linear')\n",
    "print(\"Train score: {:.2f}\".format(pca_linreg.score(X_reduced_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(pca_linreg.score(X_reduced_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the PCA Linear Regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Linear\n",
      "R2_pca_linreg: 0.51\n",
      "RMSE_pca_linreg: 0.10\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_pca_linreg = pca_linreg.predict(X_reduced_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('PCA Linear')\n",
    "R2_pca_linreg = r2_score(y_test, y_pred_pca_linreg)\n",
    "print(\"R2_pca_linreg: {:.2f}\".format(R2_pca_linreg)) \n",
    "RMSE_pca_linreg = np.sqrt(metrics.mean_squared_error(y_test, y_pred_pca_linreg))\n",
    "print('RMSE_pca_linreg: {:.2f}'.format(RMSE_pca_linreg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the R2 score and the RMSE value of the PCA Linear Regressor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. PCA - Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'fit_intercept': False, 'normalize': True}\n",
      "Best cross-validation score: -0.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# poly_1\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_reduced_train)\n",
    "X_test_poly = poly.transform(X_reduced_test)\n",
    "\n",
    "# set param_grid\n",
    "param_grid = {'fit_intercept':[True,False], \n",
    "              'normalize':[True]}\n",
    "\n",
    "# define the model\n",
    "polyreg = LinearRegression()\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# Use GridSearch\n",
    "gscv_polyreg = GridSearchCV(polyreg, \n",
    "                           param_grid,\n",
    "                           cv=cv,\n",
    "                           scoring=scoring\n",
    "                          )\n",
    "\n",
    "# Fit the model\n",
    "gscv_polyreg.fit(X_train_poly, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters: {}\".format(gscv_polyreg.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(gscv_polyreg.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Polynomial\n",
      "Train score: 1.00\n",
      "Test score: -2.99\n"
     ]
    }
   ],
   "source": [
    "# set best linear\n",
    "pca_polyreg = LinearRegression(**gscv_polyreg.best_params_)\n",
    "\n",
    "# Fit the model\n",
    "pca_polyreg.fit(X_train_poly, y_train)\n",
    "\n",
    "# train and test score\n",
    "print('PCA Polynomial')\n",
    "print(\"Train score: {:.2f}\".format(pca_polyreg.score(X_train_poly, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(pca_polyreg.score(X_test_poly, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the PCA Polynomial Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Polynomial\n",
      "R2_pca_polyreg: -2.99\n",
      "RMSE_pca_polyreg: 0.29\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_pca_polyreg = pca_polyreg.predict(X_test_poly)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('PCA Polynomial')\n",
    "R2_pca_polyreg = r2_score(y_test, y_pred_pca_polyreg)\n",
    "print(\"R2_pca_polyreg: {:.2f}\".format(R2_pca_polyreg)) \n",
    "RMSE_pca_polyreg = np.sqrt(metrics.mean_squared_error(y_test, y_pred_pca_polyreg))\n",
    "print('RMSE_pca_polyreg: {:.2f}'.format(RMSE_pca_polyreg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the R2 score and the RMSE value of the PCA Polynomial Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1094, 1225)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_poly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation score and R2 score are negative is because we face a curse of dimensionality problem using the PolynomialFeatures function with degree = 2. As the result shown above, the number of instances is 1094, and the number of features is 1225. Since the number of features exceeds the number of rows, resulting in a training score of 1. We may reduce the degree from 2 to 1; however,  we will get the same result as linear regression. Therefore, we can conclude that this Polynomial model is valueless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. PCA - Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for PCA Ridge Regression: {'alpha': 10, 'normalize': False, 'tol': 1e-06}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# set param_grid\n",
    "param_grid = {'alpha':[0.1, 1, 10, 100],\n",
    "              'normalize':[True,False], \n",
    "              'tol':[1e-06,5e-06,1e-05,5e-05]\n",
    "             }\n",
    "\n",
    "# define the model\n",
    "ridge = Ridge(random_state=0)\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_ridge= GridSearchCV(ridge, param_grid, \n",
    "                         cv=cv,\n",
    "                         scoring=scoring)\n",
    "\n",
    "# fit the grid search\n",
    "gscv_ridge.fit(X_reduced_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters for PCA Ridge Regression: {}\".format(gscv_ridge.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Ridge\n",
      "Train score: 0.79\n",
      "Test score: 0.50\n"
     ]
    }
   ],
   "source": [
    "# define the best ridge model\n",
    "pca_ridge = Ridge(alpha=gscv_ridge.best_params_['alpha'],\n",
    "                   normalize=gscv_ridge.best_params_['normalize'],\n",
    "                  tol=gscv_ridge.best_params_['tol']\n",
    "                  )\n",
    "\n",
    "# fit the model\n",
    "pca_ridge.fit(X_reduced_train, y_train)\n",
    "\n",
    "# train and test score\n",
    "print('PCA Ridge')\n",
    "print(\"Train score: {:.2f}\".format(pca_ridge.score(X_reduced_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(pca_ridge.score(X_reduced_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the PCA Ridge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Ridge\n",
      "R2_pca_ridge: 0.50\n",
      "RMSE_pca_ridge: 0.10\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_pca_ridge = pca_ridge.predict(X_reduced_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('PCA Ridge')\n",
    "R2_pca_ridge = r2_score(y_test, y_pred_pca_ridge)\n",
    "print(\"R2_pca_ridge: {:.2f}\".format(R2_pca_ridge)) \n",
    "RMSE_pca_ridge = np.sqrt(metrics.mean_squared_error(y_test, y_pred_pca_ridge))\n",
    "print('RMSE_pca_ridge: {:.2f}'.format(RMSE_pca_ridge))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the R2 score and the RMSE value of the PCA Ridge model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. PCA - Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Lasso Regression: {'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "param_grid = {'alpha':[0.001, 0.01, 0.1, 1, 10, 100, 250, 500, 1000]}\n",
    "\n",
    "# define the model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_lasso= GridSearchCV(lasso, param_grid, \n",
    "                         cv=cv,\n",
    "                         scoring=scoring,\n",
    "                         return_train_score=True)\n",
    "\n",
    "# fit the grid search\n",
    "gscv_lasso.fit(X_reduced_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters for Lasso Regression: {}\".format(gscv_lasso.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Lasso\n",
      "Train score: 0.76\n",
      "Test score: 0.47\n"
     ]
    }
   ],
   "source": [
    "# define the best lasso model\n",
    "pca_lasso = Lasso(alpha=gscv_lasso.best_params_['alpha'])\n",
    "\n",
    "# fit the model\n",
    "pca_lasso.fit(X_reduced_train, y_train)\n",
    "\n",
    "# train and test score\n",
    "print('PCA Lasso')\n",
    "print(\"Train score: {:.2f}\".format(pca_lasso.score(X_reduced_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(pca_lasso.score(X_reduced_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the PCA Lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Lasso\n",
      "R2_pca_lasso: 0.47\n",
      "RMSE_pca_lasso: 0.11\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_pca_lasso = pca_lasso.predict(X_reduced_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('PCA Lasso')\n",
    "R2_pca_lasso = r2_score(y_test, y_pred_pca_lasso)\n",
    "print(\"R2_pca_lasso: {:.2f}\".format(R2_pca_lasso)) \n",
    "RMSE_pca_lasso = np.sqrt(metrics.mean_squared_error(y_test, y_pred_pca_lasso))\n",
    "print('RMSE_pca_lasso: {:.2f}'.format(RMSE_pca_lasso))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the R2 score and the RMSE value of the PCA Lasso model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. PCA - LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for PCA LinearSVR: {'C': 0.01, 'epsilon': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "param_grid = {'C':[0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'epsilon':[0.1,0.2,0.3,0.5]}\n",
    "\n",
    "# define the model\n",
    "svr = LinearSVR(random_state=0)\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_svr= GridSearchCV(svr, param_grid, \n",
    "                       cv=cv,\n",
    "                       scoring=scoring,\n",
    "                       return_train_score=True)\n",
    "\n",
    "# fit the grid search\n",
    "gscv_svr.fit(X_reduced_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters for PCA LinearSVR: {}\".format(gscv_svr.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA LinearSVR\n",
      "Train score: 0.75\n",
      "Test score: 0.45\n"
     ]
    }
   ],
   "source": [
    "# define the best svr model\n",
    "pca_svr = LinearSVR(C=gscv_svr.best_params_['C'],\n",
    "              epsilon=gscv_svr.best_params_['epsilon'])\n",
    "\n",
    "# fit the model\n",
    "pca_svr.fit(X_reduced_train, y_train)\n",
    "\n",
    "# train and test score\n",
    "print('PCA LinearSVR')\n",
    "print(\"Train score: {:.2f}\".format(pca_svr.score(X_reduced_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(pca_svr.score(X_reduced_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the PCA LinearSVR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA LinearSVR\n",
      "R2_pca_svr: 0.45\n",
      "RMSE_pca_svr: 0.11\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_pca_svr = pca_svr.predict(X_reduced_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('PCA LinearSVR')\n",
    "R2_pca_svr = r2_score(y_test, y_pred_pca_svr)\n",
    "print(\"R2_pca_svr: {:.2f}\".format(R2_pca_svr)) \n",
    "RMSE_pca_svr = np.sqrt(metrics.mean_squared_error(y_test, y_pred_pca_svr))\n",
    "print('RMSE_pca_svr: {:.2f}'.format(RMSE_pca_svr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the R2 score and the RMSE value of the PCA LinearSVR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. PCA - SVM with linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for PCA SVM with linear kernel: {'C': 0.1, 'coef0': 0.01, 'degree': 3, 'gamma': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "param_grid = {\n",
    "              'C' : [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'degree' : [3,8],\n",
    "              'coef0' : [0.01,10,0.5],\n",
    "              'gamma' : ('auto','scale')},\n",
    "\n",
    "# define the model\n",
    "svr_linear_kernal = SVR(kernel='linear')\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_svr_kernal= GridSearchCV(svr_linear_kernal, param_grid,\n",
    "                              cv=cv,\n",
    "                              scoring=scoring,\n",
    "                              return_train_score=True)\n",
    "\n",
    "# fit the grid search\n",
    "gscv_svr_kernal.fit(X_reduced_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters for PCA SVM with linear kernel: {}\".format(gscv_svr_kernal.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA SVM with Linear\n",
      "Train score: 0.74\n",
      "Test score: 0.55\n"
     ]
    }
   ],
   "source": [
    "# define the best svr model\n",
    "pca_svr_linear = SVR(kernel='linear',\n",
    "               C=gscv_svr_kernal.best_params_['C'],\n",
    "               degree=gscv_svr_kernal.best_params_['degree'],\n",
    "               coef0=gscv_svr_kernal.best_params_['coef0'],\n",
    "               gamma=gscv_svr_kernal.best_params_['gamma'])\n",
    "\n",
    "# fit the model\n",
    "pca_svr_linear.fit(X_reduced_train, y_train)\n",
    "\n",
    "# train and test score\n",
    "print('PCA SVM with Linear')\n",
    "print(\"Train score: {:.2f}\".format(pca_svr_linear.score(X_reduced_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(pca_svr_linear.score(X_reduced_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the PCA SVM with linear kernel model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA SVM with Linear\n",
      "R2_pca_svr_linear: 0.55\n",
      "RMSE_pca_svr_linear: 0.10\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_pca_svr_linear = pca_svr_linear.predict(X_reduced_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('PCA SVM with Linear')\n",
    "R2_pca_svr_linear = r2_score(y_test, y_pred_pca_svr_linear)\n",
    "print(\"R2_pca_svr_linear: {:.2f}\".format(R2_pca_svr_linear)) \n",
    "RMSE_pca_svr_linear = np.sqrt(metrics.mean_squared_error(y_test, y_pred_pca_svr_linear))\n",
    "print('RMSE_pca_svr_linear: {:.2f}'.format(RMSE_pca_svr_linear))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the R2 score and the RMSE value of the PCA SVM with linear kernel model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. PCA - SVM with poly kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for PCA SVM with poly kernel: {'C': 0.01, 'coef0': 10, 'degree': 3, 'gamma': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "param_grid = {\n",
    "              'C' : [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'degree' : [3,8],\n",
    "              'coef0' : [0.01,1,10],\n",
    "              'gamma' : ('auto','scale')},\n",
    "\n",
    "# define the model\n",
    "svr_poly_kernal = SVR(kernel='poly')\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_svr_kernal= GridSearchCV(svr_poly_kernal, param_grid,\n",
    "                              cv=cv,\n",
    "                              scoring=scoring,\n",
    "                              return_train_score=True)\n",
    "\n",
    "# fit the grid search\n",
    "gscv_svr_kernal.fit(X_reduced_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters for PCA SVM with poly kernel: {}\".format(gscv_svr_kernal.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA SVM with Poly\n",
      "Train score: 0.75\n",
      "Test score: 0.56\n"
     ]
    }
   ],
   "source": [
    "# define the best svr model\n",
    "pca_svr_poly = SVR(kernel='poly',\n",
    "               C=gscv_svr_kernal.best_params_['C'],\n",
    "               degree=gscv_svr_kernal.best_params_['degree'],\n",
    "               coef0=gscv_svr_kernal.best_params_['coef0'],\n",
    "               gamma=gscv_svr_kernal.best_params_['gamma'])\n",
    "\n",
    "\n",
    "# fit the model\n",
    "pca_svr_poly.fit(X_reduced_train, y_train)\n",
    "\n",
    "# train and test score\n",
    "print('PCA SVM with Poly')\n",
    "print(\"Train score: {:.2f}\".format(pca_svr_poly.score(X_reduced_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(pca_svr_poly.score(X_reduced_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the PCA SVM with poly kernel model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA SVM with Poly\n",
      "R2_pca_svr_poly: 0.56\n",
      "RMSE_pca_svr_poly: 0.10\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_pca_svr_poly = pca_svr_poly.predict(X_reduced_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('PCA SVM with Poly')\n",
    "R2_pca_svr_poly = r2_score(y_test, y_pred_pca_svr_poly)\n",
    "print(\"R2_pca_svr_poly: {:.2f}\".format(R2_pca_svr_poly)) \n",
    "RMSE_pca_svr_poly = np.sqrt(metrics.mean_squared_error(y_test, y_pred_pca_svr_poly))\n",
    "print('RMSE_pca_svr_poly: {:.2f}'.format(RMSE_pca_svr_poly))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the R2 score and the RMSE value of the PCA SVM with poly kernel model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. PCA - SVM with rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for PCA SVM with rbf kernel: {'C': 1, 'coef0': 0.01, 'degree': 3, 'gamma': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "param_grid = {\n",
    "              'C' : [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'degree' : [3,8],\n",
    "              'coef0' : [0.01,10,0.5],\n",
    "              'gamma' : ('auto','scale')},\n",
    "\n",
    "# define the model\n",
    "svr_rbf_kernal = SVR(kernel='rbf')\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_svr_kernal= GridSearchCV(svr_rbf_kernal, param_grid,\n",
    "                              cv=cv,\n",
    "                              scoring=scoring,\n",
    "                              return_train_score=True)\n",
    "\n",
    "# fit the grid search\n",
    "gscv_svr_kernal.fit(X_reduced_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters for PCA SVM with rbf kernel: {}\".format(gscv_svr_kernal.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA SVM with Rbf\n",
      "Train score: 0.76\n",
      "Test score: 0.58\n"
     ]
    }
   ],
   "source": [
    "# define the best svr model\n",
    "pca_svr_rbf = SVR(kernel='rbf',\n",
    "               C=gscv_svr_kernal.best_params_['C'],\n",
    "               degree=gscv_svr_kernal.best_params_['degree'],\n",
    "               coef0=gscv_svr_kernal.best_params_['coef0'],\n",
    "               gamma=gscv_svr_kernal.best_params_['gamma'])\n",
    "\n",
    "# fit the model\n",
    "pca_svr_rbf.fit(X_reduced_train, y_train)\n",
    "\n",
    "# train and test score\n",
    "print('PCA SVM with Rbf')\n",
    "print(\"Train score: {:.2f}\".format(pca_svr_rbf.score(X_reduced_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(pca_svr_rbf.score(X_reduced_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the PCA SVM with rbf kernel model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA SVM with Rbf\n",
      "R2_pca_svr_rbf: 0.58\n",
      "RMSE_pca_svr_rbf: 0.09\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_pca_svr_rbf = pca_svr_rbf.predict(X_reduced_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('PCA SVM with Rbf')\n",
    "R2_pca_svr_rbf = r2_score(y_test, y_pred_pca_svr_rbf)\n",
    "print(\"R2_pca_svr_rbf: {:.2f}\".format(R2_pca_svr_rbf)) \n",
    "RMSE_pca_svr_rbf = np.sqrt(metrics.mean_squared_error(y_test, y_pred_pca_svr_rbf))\n",
    "print('RMSE_pca_svr_rbf: {:.2f}'.format(RMSE_pca_svr_rbf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the R2 score and the RMSE value of the PCA SVM with rbf kernel model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. PCA - Decison Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for PCA Decision Tree Regressor: {'max_depth': 5, 'max_features': 4, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "param_grid = {'max_depth': [3, 5, 10],\n",
    "              'max_features': [3, 4, 5],\n",
    "             'random_state': [0]}\n",
    "\n",
    "# define the model\n",
    "dt = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "# Define the cv\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=0)\n",
    "\n",
    "# Define the scoring\n",
    "scoring = 'r2'\n",
    "\n",
    "# define the grid search\n",
    "gscv_dt = GridSearchCV(dt, param_grid,\n",
    "                       cv=cv,\n",
    "                       scoring=scoring,\n",
    "                       return_train_score=True)\n",
    "\n",
    "# fit the grid search\n",
    "gscv_dt.fit(X_reduced_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters for PCA Decision Tree Regressor: {}\".format(gscv_dt.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Decision Tree\n",
      "Train score: 0.48\n",
      "Test score: 0.02\n"
     ]
    }
   ],
   "source": [
    "# define the best dt model\n",
    "pca_dt = DecisionTreeRegressor(\n",
    "    max_depth=gscv_dt.best_params_['max_depth'],\n",
    "    max_features=gscv_dt.best_params_['max_features'],\n",
    "    random_state=0)\n",
    "\n",
    "# fit the model\n",
    "pca_dt.fit(X_reduced_train, y_train)\n",
    "\n",
    "# train and test score\n",
    "print('PCA Decision Tree')\n",
    "print(\"Train score: {:.2f}\".format(pca_dt.score(X_reduced_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(pca_dt.score(X_reduced_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the PCA Decision Tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Decision Tree\n",
      "R2_pca_dt: 0.02\n",
      "RMSE_pca_dt: 0.14\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred_pca_dt = pca_dt.predict(X_reduced_test)\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print('PCA Decision Tree')\n",
    "R2_pca_dt = r2_score(y_test, y_pred_pca_dt)\n",
    "print(\"R2_pca_dt: {:.2f}\".format(R2_pca_dt)) \n",
    "RMSE_pca_dt = np.sqrt(metrics.mean_squared_error(y_test, y_pred_pca_dt))\n",
    "print('RMSE_pca_dt: {:.2f}'.format(RMSE_pca_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the R2 score and the RMSE value of the PCA Decision Tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results Comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before PCA / After PCA:\n",
      "\n",
      "Algorithm                 Train score    R2    RMSE    Train score (After PCA)    R2 (After PCA)    RMSE (After PCA)\n",
      "----------------------  -------------  ----  ------  -------------------------  ----------------  ------------------\n",
      "KNN                              0.69  0.29    0.12                       0.69              0.3                 0.12\n",
      "Linear                           0.84  0.57    0.1                        0.79              0.51                0.1\n",
      "Polynomial                       1     0.43    0.11                       1                -2.99                0.29\n",
      "Ridge                            0.82  0.54    0.1                        0.79              0.5                 0.1\n",
      "Lasso                            0.8   0.51    0.1                        0.76              0.47                0.11\n",
      "LinearSVR                        0.7   0.6     0.09                       0.75              0.45                0.11\n",
      "SVM with linear kernel           0.69  0.61    0.09                       0.74              0.55                0.1\n",
      "SVM with poly kernel             0.76  0.62    0.09                       0.75              0.56                0.1\n",
      "SVM with rbf kernel              0.72  0.63    0.09                       0.76              0.58                0.09\n",
      "Decision Tree                    0.92  0.27    0.12                       0.48              0.02                0.14\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "table1 = [['Algorithm','Train score', 'R2', 'RMSE',\n",
    "           'Train score (After PCA)', 'R2 (After PCA)', 'RMSE (After PCA)'], \n",
    "          ['KNN', 0.69, 0.29, 0.12, 0.69, 0.30, 0.12], \n",
    "          ['Linear', 0.84, 0.57, 0.10, 0.79, 0.51, 0.10], \n",
    "          ['Polynomial', 1.00, 0.43, 0.11, 1.00, -2.99, 0.29], \n",
    "          ['Ridge', 0.82, 0.54, 0.10, 0.79, 0.50, 0.10], \n",
    "          ['Lasso', 0.80, 0.51, 0.10, 0.76, 0.47, 0.11], \n",
    "          ['LinearSVR', 0.70, 0.60, 0.09, 0.75, 0.45, 0.11],\n",
    "          ['SVM with linear kernel', 0.69, 0.61, 0.09, 0.74, 0.55, 0.10], \n",
    "          ['SVM with poly kernel', 0.76, 0.62, 0.09, 0.75, 0.56, 0.10], \n",
    "          ['SVM with rbf kernel', 0.72, 0.63, 0.09, 0.76, 0.58, 0.09],\n",
    "          ['Decision Tree', 0.92, 0.27, 0.12, 0.48, 0.02, 0.14]]\n",
    "\n",
    "print(\"Before PCA / After PCA:\\n\")\n",
    "print(tabulate(table1, headers='firstrow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above reveals that the training score and the test score of each model after applying PCA. \n",
    "\n",
    "(1) KNN model: The R2 score is slightly better after applying PCA, and the RMSE remains the same. Therefore, we can infer that the KNN model performs better with PCA method.\n",
    "\n",
    "(2) Linear Regression: The R2 score reduces from 0.57 to 0.51 after applying PCA. Hence, we can conclude that applying PCA does not help us find a better Linear Regression model.\n",
    "\n",
    "(3) Polynomial Regression: Both the original polynomial model and the new polynomial model with PCA method face an overfitting problem as we can see both train scores are zero. Since our original training set only has 1459 rows and 105 features and the training set with PCA method has 1094 rows and 48 features, the number of features must exceed the number of rows after applying the PolynomialFeatures function to transform the dataset. As we mention before, both polynomial Regression models are bad models and valueless.\n",
    "\n",
    "(4) Ridge: The train score reduces from 0.82 to 0.79, and the R2 score reduces from 0.54 to 0.50. But the RMSE remains. Hence, we can conclude that applying PCA does not help us find a better Ridge model.\n",
    "\n",
    "(5) Lasso: The train score reduces from 0.80 to 0.76, the R2 score reduces from 0.51 to 0.47, and the RMSE increases from 0.1 to 0.11. Hence, we can conclude that applying PCA does not help us to find a better Lasso model.\n",
    "\n",
    "(6) Linear SVR: Although the training score improves from 0.70 to 0.75, the R2 score reduces from 0.60 to 0.45. Additionally, the RMSE increases from 0.09 to 0.11. Hence, we can conclude that applying PCA does not help us to find a better Linear SVR model.\n",
    "\n",
    "(7) Kernel SVM (linear): The train score improves from 0.69 to 0.74, but the R2 score reduces from 0.61 to 0.55. Also, the RMSE increases from 0.09 to 0.11. Hence, we can infer that applying PCA does not help us find a better SVM model with linear kernel.\n",
    "\n",
    "(8) Kernel SVM (poly): The train score reduces from 0.76 to 0.75, and the R2 score reduces from 0.62 to 0.56. Also, the RMSE increases from 0.09 to 0.10. Hence, we can infer that applying PCA does not help us find a better SVM model with poly kernel.\n",
    "\n",
    "(9) Kernel SVM (rbf): The train score improves from 0.72 to 0.76, but the R2 score reduces from 0.63 to 0.58. Hence, we can infer that applying PCA does not help us find a better SVM model with rbf kernel.\n",
    "\n",
    "(10) Decision Tree: The train and test scores significantly reduced, and the RMSE increases from 0.12 to 0.14. Hence, we can infer that applying PCA does not help us find the better Decision Tree model.\n",
    "\n",
    "Conclusion: We can find that almost all models get a lower test score even though some have a higher train score. Typically, PCA is particularly useful in processing data where multi-colinearity exists between the features. Therefore, we may conclude that the features in this dataset are not highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 10\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1094, 105)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'batch_size': 16, 'epochs': 50}\n"
     ]
    }
   ],
   "source": [
    "from keras import metrics\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(105, input_dim=105, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mse', optimizer='sgd' , metrics='mse')\n",
    "    return model\n",
    "\n",
    "gscv_model_keras = KerasRegressor(build_fn = create_model, verbose = 0)\n",
    "\n",
    "param_grid = {'batch_size':[16, 32, 64] , 'epochs':[25, 50]}\n",
    "\n",
    "grid_search = GridSearchCV(estimator= gscv_model_keras, \n",
    "                            param_grid = param_grid, \n",
    "                            cv = 5)\n",
    "\n",
    " # Fit the grid search\n",
    "grid_search_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print The value of best Hyperparameters\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyper-parameters for Keras Regressor is: {'batch_size': 16, 'epochs': 50}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "69/69 [==============================] - 1s 4ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0248 - val_mse: 0.0248\n",
      "Epoch 2/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0223 - val_mse: 0.0223\n",
      "Epoch 3/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0200 - val_mse: 0.0200\n",
      "Epoch 4/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0186 - val_mse: 0.0186\n",
      "Epoch 5/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0179 - val_mse: 0.0179\n",
      "Epoch 6/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.0172 - val_mse: 0.0172\n",
      "Epoch 7/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0151 - val_mse: 0.0151\n",
      "Epoch 8/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0160 - val_mse: 0.0160\n",
      "Epoch 9/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 10/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 11/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 12/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0138 - val_mse: 0.0138\n",
      "Epoch 13/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 14/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 15/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 16/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 17/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 18/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 19/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 20/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 21/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 22/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 23/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 24/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 25/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 26/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 27/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0123 - val_mse: 0.0123\n",
      "Epoch 28/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 29/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 30/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 31/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 32/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 33/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 34/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 35/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 36/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 37/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 38/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 39/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 40/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 41/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0108 - val_mse: 0.0108\n",
      "Epoch 42/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 43/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 44/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 45/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 46/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 47/50\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 48/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 49/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 50/50\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0109 - val_mse: 0.0109\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(105, input_dim=105, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mse', optimizer='sgd', metrics = 'mse')\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=grid_search.best_params_['epochs'],\n",
    "          batch_size=grid_search.best_params_['batch_size'],\n",
    "          validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_train_predict = model.predict(X_train)\n",
    "y_test_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.80\n",
      "Test score: 0.48\n",
      "RMSE score: 0.10\n"
     ]
    }
   ],
   "source": [
    "print('Train score: {:.2f}'.format(r2_score(y_train, y_train_predict)))\n",
    "print('Test score: {:.2f}'.format(r2_score(y_test, y_test_predict)))\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print('RMSE score: {:.2f}'.format(mean_squared_error(y_test, y_test_predict, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above indicates that the training score and the test score of the deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Selection:\n",
      "\n",
      "Algorithm               Train score    R2    RMSE\n",
      "--------------------  -------------  ----  ------\n",
      "KNN with Adaboost              0.88  0.41    0.11\n",
      "Linear with Adaboost           0.88  0.61    0.09\n",
      "Ridge with Bagging             0.79  0.51    0.1\n",
      "Ridge with Pasting             0.74  0.43    0.11\n",
      "Lasso with Bagging             0.81  0.52    0.1\n",
      "Lasso with Pasting             0.8   0.5     0.1\n",
      "Gradient                       0.94  0.56    0.1\n",
      "deep learning                  0.8   0.48    0.1\n",
      "\n",
      "\n",
      "Algorithm                 Train score    R2    RMSE    Train score (After PCA)    R2 (After PCA)    RMSE (After PCA)\n",
      "----------------------  -------------  ----  ------  -------------------------  ----------------  ------------------\n",
      "KNN                              0.69  0.29    0.12                       0.69              0.3                 0.12\n",
      "Linear                           0.84  0.57    0.1                        0.79              0.51                0.1\n",
      "Polynomial                       1     0.43    0.11                       1                -2.99                0.29\n",
      "Ridge                            0.82  0.54    0.1                        0.79              0.5                 0.1\n",
      "Lasso                            0.8   0.51    0.1                        0.76              0.47                0.11\n",
      "LinearSVR                        0.7   0.6     0.09                       0.75              0.45                0.11\n",
      "SVM with linear kernel           0.69  0.61    0.09                       0.74              0.55                0.1\n",
      "SVM with poly kernel             0.76  0.62    0.09                       0.75              0.56                0.1\n",
      "SVM with rbf kernel              0.72  0.63    0.09                       0.76              0.58                0.09\n",
      "Decision Tree                    0.92  0.27    0.12                       0.48              0.02                0.14\n"
     ]
    }
   ],
   "source": [
    "table2 = [['Algorithm','Train score', 'R2', 'RMSE'], \n",
    "          ['KNN with Adaboost', 0.88, 0.41, 0.11], \n",
    "          ['Linear with Adaboost', 0.88, 0.61, 0.09], \n",
    "          ['Ridge with Bagging', 0.79, 0.51, 0.10], \n",
    "          ['Ridge with Pasting', 0.74, 0.43, 0.11], \n",
    "          ['Lasso with Bagging', 0.81, 0.52, 0.10], \n",
    "          ['Lasso with Pasting', 0.80, 0.50, 0.10], \n",
    "          ['Gradient', 0.94, 0.56, 0.10], \n",
    "          ['deep learning', 0.80, 0.48, 0.10]]\n",
    "\n",
    "print(\"Model Selection:\\n\")\n",
    "print(tabulate(table2, headers='firstrow'))\n",
    "print(\"\\n\")\n",
    "print(tabulate(table1, headers='firstrow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two tables above. Some models use ensemble methods, and some models apply PCA before modeling.\n",
    "\n",
    "To find the best regression model, we choose R2 and RMSE as our evaluation indicators. The higher value of R2, the better. The lower value of RMSE implies higher accuracy of a regression model. Additionally, we prefer the difference between train scores and test scores as small as possible.\n",
    "\n",
    "To sum up, the ``SVM classifier with RBF kernel`` is the optimal model for prediction since it has the highest R2 score of 0.63 and a low RMSE value of 0.09."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model - SVM classifier with RBF kernel\n",
      "Train score: 0.76\n"
     ]
    }
   ],
   "source": [
    "# scale\n",
    "scaler = MinMaxScaler()\n",
    "## entire train set\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = (y - y.min()) / (y.max() - y.min())\n",
    "## entire test set\n",
    "X_test_scaled = scaler.transform(test_df)\n",
    "\n",
    "# define the model\n",
    "best_svr_rbf = SVR(kernel='rbf',\n",
    "               C=1,\n",
    "               degree=3,\n",
    "               coef0=0.01,\n",
    "               gamma='auto')\n",
    "\n",
    "# fit the model\n",
    "best_svr_rbf.fit(X_scaled, y_scaled)\n",
    "\n",
    "# train score\n",
    "print(\"The best model - SVM classifier with RBF kernel\")\n",
    "print(\"Train score: {:.2f}\".format(best_svr_rbf.score(X_scaled, y_scaled)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From project 1, we know the best hyper-parameters of SVM with RBF kernel model using Grid Search is {'C': 1, 'coef0': 0.01, 'degree': 3, 'gamma': 'auto'}, so we use them to build our model. Finally, we train the model on the entire dataset to predict 1459 house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = best_svr_rbf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>114767.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>189034.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>182759.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>218839.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>281192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1466</td>\n",
       "      <td>176882.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1467</td>\n",
       "      <td>220317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1468</td>\n",
       "      <td>167245.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1469</td>\n",
       "      <td>205072.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1470</td>\n",
       "      <td>129000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id  SalePrice\n",
       "0  1461   114767.0\n",
       "1  1462   189034.0\n",
       "2  1463   182759.0\n",
       "3  1464   218839.0\n",
       "4  1465   281192.0\n",
       "5  1466   176882.0\n",
       "6  1467   220317.0\n",
       "7  1468   167245.0\n",
       "8  1469   205072.0\n",
       "9  1470   129000.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SalePrice_prediction = pd.DataFrame(columns=['Id', 'SalePrice'])\n",
    "SalePrice_prediction['Id'] = test_df_forPred['Id']\n",
    "SalePrice_prediction['SalePrice'] = predictions * (y.max() - y.min()) + y.min()\n",
    "SalePrice_prediction['SalePrice'] = round(SalePrice_prediction['SalePrice'], 0)\n",
    "SalePrice_prediction.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows the 10 predicted house prices based on ID with the best model ``SVM classifier with RBF kernel``. We also save the entire sale prices prediction into a CSV file 'SalePrice_prediction_Project2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "SalePrice_prediction.to_csv('SalePrice_prediction_Project2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}