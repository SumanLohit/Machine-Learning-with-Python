{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset\r\n",
    "This data was collected via a survey on Amazon Mechanical Turk. The survey describes different driving scenarios including the destination, current time, weather, passenger, etc., and then ask the person whether he will accept the coupon if he is the driver. For more information about the dataset, please refer to the paper:\r\n",
    "Wang, Tong, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and Perry MacNeille. 'A bayesian framework for learning rule sets for interpretable classification.' The Journal of Machine Learning Research 18, no. 1 (2017): 2357-2393.\r\n",
    "\r\n",
    "\r\n",
    "#### Attribute Information:\r\n",
    "\r\n",
    "* destination: No Urgent Place, Home, Work\r\n",
    "* passanger: Alone, Friend(s), Kid(s), Partner (who are the passengers in the car)\r\n",
    "* weather: Sunny, Rainy, Snowy\r\n",
    "* temperature:55, 80, 30\r\n",
    "* time: 2PM, 10AM, 6PM, 7AM, 10PM\r\n",
    "* coupon: Restaurant(<$\\$$20), Coffee House, Carry out & Take away, Bar, Restaurant($\\$$20-$\\$$50)\r\n",
    "* expiration: 1d, 2h (the coupon expires in 1 day or in 2 hours)\r\n",
    "* gender: Female, Male\r\n",
    "* age: 21, 46, 26, 31, 41, 50plus, 36, below21\r\n",
    "* maritalStatus: Unmarried partner, Single, Married partner, Divorced, Widowed\r\n",
    "* has_Children:1, 0\r\n",
    "* education: Some college - no degree, Bachelors degree, Associates degree, High School Graduate, Graduate degree (Masters or Doctorate), Some High School\r\n",
    "* occupation: Unemployed, Architecture & Engineering, Student, Education&Training&Library, Healthcare Support, Healthcare Practitioners & Technical, Sales & Related, Management, Arts Design Entertainment Sports & Media, Computer & Mathematical, Life Physical Social Science, Personal Care & Service, Community & Social Services, Office & Administrative Support, Construction & Extraction, Legal, Retired, Installation Maintenance & Repair, Transportation & Material Moving, Business & Financial, Protective Service, Food Preparation & Serving Related, Production Occupations, Building & Grounds Cleaning & Maintenance, Farming Fishing & Forestry\r\n",
    "* income: $\\$$37500 - $\\$$49999, $\\$$62500 - $\\$$74999, $\\$$12500 - $\\$$24999, $\\$$75000 - $\\$$87499, $\\$$50000 - $\\$$62499, $\\$$25000 - $\\$$37499, $\\$$100000 or More, $\\$$87500 - $\\$$99999, Less than $\\$$12500\r\n",
    "* Bar: never, less1, 1\\~3, gt8, nan, 4\\~8 (feature meaning: how many times do you go to a bar every month?)\r\n",
    "* CoffeeHouse: never, less1, 4\\~8, 1\\~3, gt8, nan (feature meaning: how many times do you go to a coffeehouse every month?)\r\n",
    "* CarryAway:n4\\~8, 1\\~3, gt8, less1, never (feature meaning: how many times do you get take-away food every month?)\r\n",
    "* RestaurantLessThan20: 4\\~8, 1\\~3, less1, gt8, never (feature meaning: how many times do you go to a restaurant with an average expense per person of less than $\\$$20 every month?)\r\n",
    "* Restaurant20To50: 1\\~3, less1, never, gt8, 4\\~8, nan (feature meaning: how many times do you go to a restaurant with average expense per person of $\\$$20 - $\\$$50 every month?)\r\n",
    "* toCoupon_GEQ15min:0,1 (feature meaning: driving distance to the restaurant/bar for using the coupon is greater than 15 minutes)\r\n",
    "* toCoupon_GEQ25min:0, 1 (feature meaning: driving distance to the restaurant/bar for using the coupon is greater than 25 minutes)\r\n",
    "* direction_same:0, 1 (feature meaning: whether the restaurant/bar is in the same direction as your current destination)\r\n",
    "* direction_opp:1, 0 (feature meaning: whether the restaurant/bar is in the same direction as your current destination)\r\n",
    "* Y:1, 0 (whether the coupon is accepted)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing ``train.csv`` "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "source": [
    "train = pd.read_csv(\"train.csv\")\r\n",
    "train.shape\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2378, 26)"
      ]
     },
     "metadata": {},
     "execution_count": 360
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we find how many null values are in the dataset. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "source": [
    "train.isna().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "destination                0\n",
       "passanger                  0\n",
       "weather                   47\n",
       "temperature               34\n",
       "time                       0\n",
       "coupon                     0\n",
       "expiration                 0\n",
       "gender                    49\n",
       "age                       52\n",
       "maritalStatus              0\n",
       "has_children              46\n",
       "education                  0\n",
       "occupation                 0\n",
       "income                     0\n",
       "car                     2362\n",
       "Bar                       20\n",
       "CoffeeHouse               40\n",
       "CarryAway                 23\n",
       "RestaurantLessThan20      26\n",
       "Restaurant20To50          27\n",
       "toCoupon_GEQ5min           0\n",
       "toCoupon_GEQ15min          0\n",
       "toCoupon_GEQ25min          0\n",
       "direction_same             0\n",
       "direction_opp              0\n",
       "Y                          0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 361
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The variable 'car' has over 80% missing values. Therefore I am dropping the column. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "source": [
    "train.drop('car',axis=1,inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "source": [
    "train.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2378 entries, 0 to 2377\n",
      "Data columns (total 25 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   destination           2378 non-null   object \n",
      " 1   passanger             2378 non-null   object \n",
      " 2   weather               2331 non-null   object \n",
      " 3   temperature           2344 non-null   float64\n",
      " 4   time                  2378 non-null   object \n",
      " 5   coupon                2378 non-null   object \n",
      " 6   expiration            2378 non-null   object \n",
      " 7   gender                2329 non-null   object \n",
      " 8   age                   2326 non-null   object \n",
      " 9   maritalStatus         2378 non-null   object \n",
      " 10  has_children          2332 non-null   float64\n",
      " 11  education             2378 non-null   object \n",
      " 12  occupation            2378 non-null   object \n",
      " 13  income                2378 non-null   object \n",
      " 14  Bar                   2358 non-null   object \n",
      " 15  CoffeeHouse           2338 non-null   object \n",
      " 16  CarryAway             2355 non-null   object \n",
      " 17  RestaurantLessThan20  2352 non-null   object \n",
      " 18  Restaurant20To50      2351 non-null   object \n",
      " 19  toCoupon_GEQ5min      2378 non-null   int64  \n",
      " 20  toCoupon_GEQ15min     2378 non-null   int64  \n",
      " 21  toCoupon_GEQ25min     2378 non-null   int64  \n",
      " 22  direction_same        2378 non-null   int64  \n",
      " 23  direction_opp         2378 non-null   int64  \n",
      " 24  Y                     2378 non-null   int64  \n",
      "dtypes: float64(2), int64(6), object(17)\n",
      "memory usage: 464.6+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "source": [
    "train.income.value_counts(dropna=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "$25000 - $37499     379\n",
       "$100000 or More     344\n",
       "$12500 - $24999     341\n",
       "$37500 - $49999     299\n",
       "$50000 - $62499     287\n",
       "Less than $12500    221\n",
       "$62500 - $74999     177\n",
       "$75000 - $87499     174\n",
       "$87500 - $99999     156\n",
       "Name: income, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 364
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "source": [
    "train.direction_opp.value_counts(dropna=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    1845\n",
       "0     533\n",
       "Name: direction_opp, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 365
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "source": [
    "train.temperature.value_counts(dropna=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "80.0     1138\n",
       "55.0      711\n",
       "30.0      436\n",
       "999.0      59\n",
       "NaN        34\n",
       "Name: temperature, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 366
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One of the categories is 999 which as a temperature does not make practical sense. Therefore, I will treat it as a null value and replace it with the most frequently occurring category, 80.0 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "source": [
    "train.temperature = train.temperature.replace(999,np.nan)\r\n",
    "train.temperature.value_counts(dropna=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "80.0    1138\n",
       "55.0     711\n",
       "30.0     436\n",
       "NaN       93\n",
       "Name: temperature, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 367
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "source": [
    "train.gender.value_counts(dropna=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Female    1214\n",
       "Male      1115\n",
       "NaN         49\n",
       "Name: gender, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 368
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "source": [
    "train.age.value_counts(dropna=False)        # 8 categories"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26         486\n",
       "21         477\n",
       "31         378\n",
       "50plus     329\n",
       "36         222\n",
       "41         212\n",
       "46         128\n",
       "below21     94\n",
       "NaN         52\n",
       "Name: age, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 369
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "source": [
    "train.has_children.value_counts(dropna=False) "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0    1353\n",
       "1.0     979\n",
       "NaN      46\n",
       "Name: has_children, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 370
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "source": [
    "train.Bar.value_counts(dropna=False)      # 5 categories"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "never    955\n",
       "less1    645\n",
       "1~3      478\n",
       "4~8      217\n",
       "gt8       63\n",
       "NaN       20\n",
       "Name: Bar, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 371
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "source": [
    "train.CoffeeHouse.value_counts(dropna=False)     # 5 categories"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "less1    629\n",
       "1~3      599\n",
       "never    562\n",
       "4~8      339\n",
       "gt8      209\n",
       "NaN       40\n",
       "Name: CoffeeHouse, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 372
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "source": [
    "train.CarryAway.value_counts(dropna=False)     # 5 categories"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1~3      875\n",
       "4~8      797\n",
       "less1    343\n",
       "gt8      313\n",
       "never     27\n",
       "NaN       23\n",
       "Name: CarryAway, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 373
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "source": [
    "train.RestaurantLessThan20.value_counts(dropna=False)     # 5 categories"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1~3      1032\n",
       "4~8       653\n",
       "less1     382\n",
       "gt8       248\n",
       "never      37\n",
       "NaN        26\n",
       "Name: RestaurantLessThan20, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 374
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "source": [
    "train.Restaurant20To50.value_counts(dropna=False)     # 5 categories\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "less1    1129\n",
       "1~3       636\n",
       "never     404\n",
       "4~8       129\n",
       "gt8        53\n",
       "NaN        27\n",
       "Name: Restaurant20To50, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 375
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I will now replace the null values with most frequently occurring value in all the columns (with null values) since they are all categorical in nature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "source": [
    "# converting np.nan to most frequently occurring value\r\n",
    "\r\n",
    "for col in train.columns:\r\n",
    "    if col in ['weather','temperature', 'gender', 'age', 'has_children', 'Bar', 'CoffeeHouse', 'CarryAway','RestaurantLessThan20','Restaurant20To50'] :\r\n",
    "        mode= train[col].mode()[0]\r\n",
    "        train[col].replace(np.nan, mode, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The column 'has_children' is being converted to an integer type. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "source": [
    "# change the data type of has_children from float to int\r\n",
    "train.has_children = train.has_children.astype('int64')\r\n",
    "train.has_children.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    1399\n",
       "1     979\n",
       "Name: has_children, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 377
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Transformation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "source": [
    "train.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2378 entries, 0 to 2377\n",
      "Data columns (total 25 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   destination           2378 non-null   object \n",
      " 1   passanger             2378 non-null   object \n",
      " 2   weather               2378 non-null   object \n",
      " 3   temperature           2378 non-null   float64\n",
      " 4   time                  2378 non-null   object \n",
      " 5   coupon                2378 non-null   object \n",
      " 6   expiration            2378 non-null   object \n",
      " 7   gender                2378 non-null   object \n",
      " 8   age                   2378 non-null   object \n",
      " 9   maritalStatus         2378 non-null   object \n",
      " 10  has_children          2378 non-null   int64  \n",
      " 11  education             2378 non-null   object \n",
      " 12  occupation            2378 non-null   object \n",
      " 13  income                2378 non-null   object \n",
      " 14  Bar                   2378 non-null   object \n",
      " 15  CoffeeHouse           2378 non-null   object \n",
      " 16  CarryAway             2378 non-null   object \n",
      " 17  RestaurantLessThan20  2378 non-null   object \n",
      " 18  Restaurant20To50      2378 non-null   object \n",
      " 19  toCoupon_GEQ5min      2378 non-null   int64  \n",
      " 20  toCoupon_GEQ15min     2378 non-null   int64  \n",
      " 21  toCoupon_GEQ25min     2378 non-null   int64  \n",
      " 22  direction_same        2378 non-null   int64  \n",
      " 23  direction_opp         2378 non-null   int64  \n",
      " 24  Y                     2378 non-null   int64  \n",
      "dtypes: float64(1), int64(7), object(17)\n",
      "memory usage: 464.6+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "source": [
    "train.Restaurant20To50.value_counts(dropna=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "less1    1156\n",
       "1~3       636\n",
       "never     404\n",
       "4~8       129\n",
       "gt8        53\n",
       "Name: Restaurant20To50, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 379
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "source": [
    "train.toCoupon_GEQ5min.value_counts(dropna=False)     # only one category. It can be dropped"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    2378\n",
       "Name: toCoupon_GEQ5min, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 380
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It can be noticed that the column 'toCoupon_GEQ5min' has only one category and doesn't add any value to the data. Hence It will be dropped."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "source": [
    "print(train.occupation.value_counts(dropna=False))   # too many categories "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unemployed                                   363\n",
      "Student                                      297\n",
      "Computer & Mathematical                      270\n",
      "Sales & Related                              216\n",
      "Education&Training&Library                   172\n",
      "Management                                   161\n",
      "Arts Design Entertainment Sports & Media     116\n",
      "Office & Administrative Support              109\n",
      "Business & Financial                         103\n",
      "Retired                                      100\n",
      "Food Preparation & Serving Related            58\n",
      "Healthcare Support                            44\n",
      "Healthcare Practitioners & Technical          44\n",
      "Community & Social Services                   43\n",
      "Legal                                         38\n",
      "Transportation & Material Moving              37\n",
      "Architecture & Engineering                    36\n",
      "Construction & Extraction                     29\n",
      "Personal Care & Service                       28\n",
      "Life Physical Social Science                  27\n",
      "Protective Service                            25\n",
      "Installation Maintenance & Repair             22\n",
      "Production Occupations                        18\n",
      "Farming Fishing & Forestry                    12\n",
      "Building & Grounds Cleaning & Maintenance     10\n",
      "Name: occupation, dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are about 25 categories in the column 'Occupation'. Hence it will be dropped."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "source": [
    "print(train.direction_same.value_counts(dropna=False))\r\n",
    "print(train.direction_opp.value_counts(dropna=False)) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0    1845\n",
      "1     533\n",
      "Name: direction_same, dtype: int64\n",
      "1    1845\n",
      "0     533\n",
      "Name: direction_opp, dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "source": [
    "# 'direction_same' and 'direction_opp' variables have reversed values. So I check if the data is reversed.\r\n",
    "\r\n",
    "(train.direction_opp == train.direction_same).sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 383
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is observed that the two variables 'direction_opp' and 'direction_same' convey the same data and one of these can be dropped. Therefore I am dropping the variable 'direction_opp'. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "source": [
    "# Drop occupation and direction_opp, toCoupon_GEQ5min\r\n",
    "\r\n",
    "train.drop(['occupation','direction_opp', 'toCoupon_GEQ5min'], axis=1,inplace=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We perform one-hot encoding for the following multiclass categorical variables that can not be ordered in any way:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "source": [
    "#one hot encoding\r\n",
    "\r\n",
    "for col2 in ['destination','passanger','weather', 'coupon', 'maritalStatus']:\r\n",
    "    cols = pd.get_dummies(train[col2], prefix= col2)\r\n",
    "    train[cols.columns] = cols\r\n",
    "    train.drop(col2, axis = 1, inplace = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "source": [
    "train.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2378 entries, 0 to 2377\n",
      "Data columns (total 37 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   temperature                      2378 non-null   float64\n",
      " 1   time                             2378 non-null   object \n",
      " 2   expiration                       2378 non-null   object \n",
      " 3   gender                           2378 non-null   object \n",
      " 4   age                              2378 non-null   object \n",
      " 5   has_children                     2378 non-null   int64  \n",
      " 6   education                        2378 non-null   object \n",
      " 7   income                           2378 non-null   object \n",
      " 8   Bar                              2378 non-null   object \n",
      " 9   CoffeeHouse                      2378 non-null   object \n",
      " 10  CarryAway                        2378 non-null   object \n",
      " 11  RestaurantLessThan20             2378 non-null   object \n",
      " 12  Restaurant20To50                 2378 non-null   object \n",
      " 13  toCoupon_GEQ15min                2378 non-null   int64  \n",
      " 14  toCoupon_GEQ25min                2378 non-null   int64  \n",
      " 15  direction_same                   2378 non-null   int64  \n",
      " 16  Y                                2378 non-null   int64  \n",
      " 17  destination_Home                 2378 non-null   uint8  \n",
      " 18  destination_No Urgent Place      2378 non-null   uint8  \n",
      " 19  destination_Work                 2378 non-null   uint8  \n",
      " 20  passanger_Alone                  2378 non-null   uint8  \n",
      " 21  passanger_Friend(s)              2378 non-null   uint8  \n",
      " 22  passanger_Kid(s)                 2378 non-null   uint8  \n",
      " 23  passanger_Partner                2378 non-null   uint8  \n",
      " 24  weather_Rainy                    2378 non-null   uint8  \n",
      " 25  weather_Snowy                    2378 non-null   uint8  \n",
      " 26  weather_Sunny                    2378 non-null   uint8  \n",
      " 27  coupon_Bar                       2378 non-null   uint8  \n",
      " 28  coupon_Carry out & Take away     2378 non-null   uint8  \n",
      " 29  coupon_Coffee House              2378 non-null   uint8  \n",
      " 30  coupon_Restaurant(20-50)         2378 non-null   uint8  \n",
      " 31  coupon_Restaurant(<20)           2378 non-null   uint8  \n",
      " 32  maritalStatus_Divorced           2378 non-null   uint8  \n",
      " 33  maritalStatus_Married partner    2378 non-null   uint8  \n",
      " 34  maritalStatus_Single             2378 non-null   uint8  \n",
      " 35  maritalStatus_Unmarried partner  2378 non-null   uint8  \n",
      " 36  maritalStatus_Widowed            2378 non-null   uint8  \n",
      "dtypes: float64(1), int64(5), object(11), uint8(20)\n",
      "memory usage: 362.4+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I will now map all the columns with ordinal categories to numerical values and transform the existing columns with the mapping."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The columns 'Bar', 'CoffeeHouse','CarryAway','RestaurantLessThan20' and 'Restaurant20To50' share the same categories and can be transformed using the same mapping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "source": [
    "# Ordinal Mapping\r\n",
    "Mapper = {\"never\":0,\"less1\":1,\"1~3\":2,\"4~8\":3,\"gt8\":4}\r\n",
    "train= train.replace({'Bar': Mapper})\r\n",
    "train= train.replace({'CoffeeHouse': Mapper})\r\n",
    "train= train.replace({'CarryAway': Mapper})\r\n",
    "train= train.replace({'RestaurantLessThan20': Mapper})\r\n",
    "train= train.replace({'Restaurant20To50': Mapper})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "source": [
    "train.income.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "$25000 - $37499     379\n",
       "$100000 or More     344\n",
       "$12500 - $24999     341\n",
       "$37500 - $49999     299\n",
       "$50000 - $62499     287\n",
       "Less than $12500    221\n",
       "$62500 - $74999     177\n",
       "$75000 - $87499     174\n",
       "$87500 - $99999     156\n",
       "Name: income, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 388
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Temperature column has only three values. Hence it can be transformed as ordinal categorical variable. <br>\n",
    "Time of the day can be ordered as such so I'm mapping Time ordinally. <br>\n",
    "Age can be mapped ordinally based on ascending order of ages.<br>\n",
    "Income intervals can also be mapped Ordinally based on the ascending order of the income ranges."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "source": [
    "Mapper2= {'30':0, '55.0': 1, '80.0': 2 }\r\n",
    "train= train.replace({'temperature': Mapper})\r\n",
    "\r\n",
    "Mapper3= {'7AM':0, '10AM': 1, '2PM': 2 , '6PM': 3, '10PM': 4}\r\n",
    "train= train.replace({'time': Mapper3})\r\n",
    "\r\n",
    "Mapper4= {'below21':0, '21': 1, '26': 2 , '31': 3, '36': 4, '41': 5, '46': 6, '50plus': 7}\r\n",
    "train= train.replace({'age': Mapper4})\r\n",
    "\r\n",
    "Mapper5= {'Less than $12500':0, '$12500 - $24999': 1, '$25000 - $37499': 2 , '$37500 - $49999': 3, '$50000 - $62499': 4, '$62500 - $74999': 5, '$75000 - $87499': 6, '$87500 - $99999': 7, '$100000 or More': 8}\r\n",
    "train= train.replace({'income': Mapper5})\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "source": [
    "train.expiration.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1d    1329\n",
       "2h    1049\n",
       "Name: expiration, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 390
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1d has the higher frequency and hence is mapped to 1 and 2h to 0."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "source": [
    "train.expiration.replace(('1d', '2h'), (1,0), inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "source": [
    "train.gender.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Female    1263\n",
       "Male      1115\n",
       "Name: gender, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 392
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Female has the higher frequency and hence is mapped to 1 and male to 0."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "source": [
    "train.gender.replace(('Female', 'Male'), (1,0), inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "source": [
    "train.education.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Bachelors degree                          805\n",
       "Some college - no degree                  785\n",
       "Graduate degree (Masters or Doctorate)    373\n",
       "Associates degree                         220\n",
       "High School Graduate                      177\n",
       "Some High School                           18\n",
       "Name: education, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 394
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the variable Education, I am binning the values and mapping them ordinally to make the variable more machine learning friendly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "source": [
    "Mapper6= {'Some High School':0, 'High School Graduate': 0, 'Some college - no degree': 1, 'Associates degree': 2, 'Bachelors degree': 2, 'Graduate degree (Masters or Doctorate)': 3}\r\n",
    "train= train.replace({'education': Mapper6})\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "source": [
    "train.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2378 entries, 0 to 2377\n",
      "Data columns (total 37 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   temperature                      2378 non-null   float64\n",
      " 1   time                             2378 non-null   int64  \n",
      " 2   expiration                       2378 non-null   int64  \n",
      " 3   gender                           2378 non-null   int64  \n",
      " 4   age                              2378 non-null   int64  \n",
      " 5   has_children                     2378 non-null   int64  \n",
      " 6   education                        2378 non-null   int64  \n",
      " 7   income                           2378 non-null   int64  \n",
      " 8   Bar                              2378 non-null   int64  \n",
      " 9   CoffeeHouse                      2378 non-null   int64  \n",
      " 10  CarryAway                        2378 non-null   int64  \n",
      " 11  RestaurantLessThan20             2378 non-null   int64  \n",
      " 12  Restaurant20To50                 2378 non-null   int64  \n",
      " 13  toCoupon_GEQ15min                2378 non-null   int64  \n",
      " 14  toCoupon_GEQ25min                2378 non-null   int64  \n",
      " 15  direction_same                   2378 non-null   int64  \n",
      " 16  Y                                2378 non-null   int64  \n",
      " 17  destination_Home                 2378 non-null   uint8  \n",
      " 18  destination_No Urgent Place      2378 non-null   uint8  \n",
      " 19  destination_Work                 2378 non-null   uint8  \n",
      " 20  passanger_Alone                  2378 non-null   uint8  \n",
      " 21  passanger_Friend(s)              2378 non-null   uint8  \n",
      " 22  passanger_Kid(s)                 2378 non-null   uint8  \n",
      " 23  passanger_Partner                2378 non-null   uint8  \n",
      " 24  weather_Rainy                    2378 non-null   uint8  \n",
      " 25  weather_Snowy                    2378 non-null   uint8  \n",
      " 26  weather_Sunny                    2378 non-null   uint8  \n",
      " 27  coupon_Bar                       2378 non-null   uint8  \n",
      " 28  coupon_Carry out & Take away     2378 non-null   uint8  \n",
      " 29  coupon_Coffee House              2378 non-null   uint8  \n",
      " 30  coupon_Restaurant(20-50)         2378 non-null   uint8  \n",
      " 31  coupon_Restaurant(<20)           2378 non-null   uint8  \n",
      " 32  maritalStatus_Divorced           2378 non-null   uint8  \n",
      " 33  maritalStatus_Married partner    2378 non-null   uint8  \n",
      " 34  maritalStatus_Single             2378 non-null   uint8  \n",
      " 35  maritalStatus_Unmarried partner  2378 non-null   uint8  \n",
      " 36  maritalStatus_Widowed            2378 non-null   uint8  \n",
      "dtypes: float64(1), int64(16), uint8(20)\n",
      "memory usage: 362.4 KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There does not seem to be any data leakage as none of the columns provide information about the target variable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing ``test.csv`` "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Final test data set is imported from test.csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "source": [
    "test = pd.read_csv(\"test.csv\")\r\n",
    "test.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(793, 25)"
      ]
     },
     "metadata": {},
     "execution_count": 397
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "source": [
    "test.isna().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "destination               0\n",
       "passanger                 0\n",
       "weather                  14\n",
       "temperature              13\n",
       "time                      0\n",
       "coupon                    0\n",
       "expiration                0\n",
       "gender                   14\n",
       "age                      12\n",
       "maritalStatus             0\n",
       "has_children             16\n",
       "education                 0\n",
       "occupation                0\n",
       "income                    0\n",
       "car                     784\n",
       "Bar                       9\n",
       "CoffeeHouse              16\n",
       "CarryAway                13\n",
       "RestaurantLessThan20      8\n",
       "Restaurant20To50         13\n",
       "toCoupon_GEQ5min          0\n",
       "toCoupon_GEQ15min         0\n",
       "toCoupon_GEQ25min         0\n",
       "direction_same            0\n",
       "direction_opp             0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 398
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similar operations done to train dataset are performed on the test Data set as well. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "source": [
    "test.drop('car',axis=1,inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "source": [
    "test.temperature.value_counts(dropna=False)\r\n",
    "test.temperature = test.temperature.replace(999,np.nan)\r\n",
    "test.temperature.value_counts(dropna=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "80.0    390\n",
       "55.0    223\n",
       "30.0    144\n",
       "NaN      36\n",
       "Name: temperature, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 400
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "source": [
    "for col in test.columns:\r\n",
    "    if col in ['weather','temperature', 'gender', 'age', 'has_children', 'Bar', 'CoffeeHouse', 'CarryAway','RestaurantLessThan20','Restaurant20To50'] :\r\n",
    "        mode= test[col].mode()[0]\r\n",
    "        test[col].replace(np.nan, mode, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "source": [
    "# change the data type of has_children from float to int\r\n",
    "test.has_children = test.has_children.astype('int64')\r\n",
    "test.has_children.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    453\n",
       "1    340\n",
       "Name: has_children, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 402
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "source": [
    "# Drop occupation and direction_opp, toCoupon_GEQ5min\r\n",
    "test.drop(['occupation','direction_opp', 'toCoupon_GEQ5min'], axis=1,inplace=True)\r\n",
    "\r\n",
    "#one hot encoding\r\n",
    "\r\n",
    "for col2 in ['destination','passanger','weather', 'coupon', 'maritalStatus']:\r\n",
    "    cols = pd.get_dummies(test[col2], prefix= col2)\r\n",
    "    test[cols.columns] = cols\r\n",
    "    test.drop(col2, axis = 1, inplace = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "source": [
    "# Ordinal Mapping\r\n",
    "Mapper = {\"never\":0,\"less1\":1,\"1~3\":2,\"4~8\":3,\"gt8\":4}\r\n",
    "test= test.replace({'Bar': Mapper})\r\n",
    "test= test.replace({'CoffeeHouse': Mapper})\r\n",
    "test= test.replace({'CarryAway': Mapper})\r\n",
    "test= test.replace({'RestaurantLessThan20': Mapper})\r\n",
    "test= test.replace({'Restaurant20To50': Mapper})\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "source": [
    "Mapper2= {'30':0, '55.0': 1, '80.0': 2 }\r\n",
    "test= test.replace({'temperature': Mapper})\r\n",
    "\r\n",
    "Mapper3= {'7AM':0, '10AM': 1, '2PM': 2 , '6PM': 3, '10PM': 4}\r\n",
    "test= test.replace({'time': Mapper3})\r\n",
    "\r\n",
    "Mapper4= {'below21':0, '21': 1, '26': 2 , '31': 3, '36': 4, '41': 5, '46': 6, '50plus': 7}\r\n",
    "test= test.replace({'age': Mapper4})\r\n",
    "\r\n",
    "Mapper5= {'Less than $12500':0, '$12500 - $24999': 1, '$25000 - $37499': 2 , '$37500 - $49999': 3, '$50000 - $62499': 4, '$62500 - $74999': 5, '$75000 - $87499': 6, '$87500 - $99999': 7, '$100000 or More': 8}\r\n",
    "test= test.replace({'income': Mapper5})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "source": [
    "test.expiration.replace(('1d', '2h'), (1,0), inplace=True)\r\n",
    "test.gender.replace(('Female', 'Male'), (1,0), inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "source": [
    "Mapper6= {'Some High School':0, 'High School Graduate': 0, 'Some college - no degree': 1, 'Associates degree': 2, 'Bachelors degree': 2, 'Graduate degree (Masters or Doctorate)': 3}\r\n",
    "test= test.replace({'education': Mapper6})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "source": [
    "test.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 793 entries, 0 to 792\n",
      "Data columns (total 36 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   temperature                      793 non-null    float64\n",
      " 1   time                             793 non-null    int64  \n",
      " 2   expiration                       793 non-null    int64  \n",
      " 3   gender                           793 non-null    int64  \n",
      " 4   age                              793 non-null    int64  \n",
      " 5   has_children                     793 non-null    int64  \n",
      " 6   education                        793 non-null    int64  \n",
      " 7   income                           793 non-null    int64  \n",
      " 8   Bar                              793 non-null    int64  \n",
      " 9   CoffeeHouse                      793 non-null    int64  \n",
      " 10  CarryAway                        793 non-null    int64  \n",
      " 11  RestaurantLessThan20             793 non-null    int64  \n",
      " 12  Restaurant20To50                 793 non-null    int64  \n",
      " 13  toCoupon_GEQ15min                793 non-null    int64  \n",
      " 14  toCoupon_GEQ25min                793 non-null    int64  \n",
      " 15  direction_same                   793 non-null    int64  \n",
      " 16  destination_Home                 793 non-null    uint8  \n",
      " 17  destination_No Urgent Place      793 non-null    uint8  \n",
      " 18  destination_Work                 793 non-null    uint8  \n",
      " 19  passanger_Alone                  793 non-null    uint8  \n",
      " 20  passanger_Friend(s)              793 non-null    uint8  \n",
      " 21  passanger_Kid(s)                 793 non-null    uint8  \n",
      " 22  passanger_Partner                793 non-null    uint8  \n",
      " 23  weather_Rainy                    793 non-null    uint8  \n",
      " 24  weather_Snowy                    793 non-null    uint8  \n",
      " 25  weather_Sunny                    793 non-null    uint8  \n",
      " 26  coupon_Bar                       793 non-null    uint8  \n",
      " 27  coupon_Carry out & Take away     793 non-null    uint8  \n",
      " 28  coupon_Coffee House              793 non-null    uint8  \n",
      " 29  coupon_Restaurant(20-50)         793 non-null    uint8  \n",
      " 30  coupon_Restaurant(<20)           793 non-null    uint8  \n",
      " 31  maritalStatus_Divorced           793 non-null    uint8  \n",
      " 32  maritalStatus_Married partner    793 non-null    uint8  \n",
      " 33  maritalStatus_Single             793 non-null    uint8  \n",
      " 34  maritalStatus_Unmarried partner  793 non-null    uint8  \n",
      " 35  maritalStatus_Widowed            793 non-null    uint8  \n",
      "dtypes: float64(1), int64(15), uint8(20)\n",
      "memory usage: 114.7 KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We notice that all the null values have been dealt with. All columns transformed to numerical values with appropriate transformations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Machine learning models "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First the train dataset is split into X_train X_test, Y_train and Y_test. Then, MinMaxScaler is used to scale the dataset. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We split the Preprocessed data into Target (Y) and feature (X) datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "source": [
    "colIndexes= train.drop(['Y'], axis=1).columns\r\n",
    "X= train[colIndexes]\r\n",
    "Y= train.Y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "X and Y datasets are split into training and test datasets individually. We use a MinMaxScaler to transform the features to avoid any distortions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "source": [
    "X_train_org, X_test_org, Y_train, Y_test= train_test_split(X,Y,random_state=0)\r\n",
    "scaler= MinMaxScaler()\r\n",
    "X_train= scaler.fit_transform(X_train_org)\r\n",
    "X_test= scaler.transform(X_test_org)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification Models\n",
    "\n",
    "We now build various Classification Models paired with a Gridsearch Cross validation to find the optimal hyperparameters. \n",
    "Since we are more concerned with finding most 'yes' values correctly even if it means a few 'no's are falsely classified as 'yes', our evaluation criterion is going to be **\"RECALL\"**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "source": [
    "# KNN CLASSIFIER\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "grid_params= {'n_neighbors': [3,4,5,8,10,15,20],'weights':['distance'],'metric':['euclidean']}\r\n",
    "\r\n",
    "gs_knn= GridSearchCV(KNeighborsClassifier(), grid_params, cv= 5 ,scoring = 'recall',return_train_score=True, verbose=1)\r\n",
    "gs_results= gs_knn.fit(X_train, Y_train)\r\n",
    "print(\"Best parameters: {}\".format(gs_results.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(gs_results.best_score_))\r\n",
    "\r\n",
    "gs_best_knn = KNeighborsClassifier(n_neighbors = gs_results.best_params_['n_neighbors'],metric=gs_results.best_params_['metric'],weights=gs_results.best_params_['weights'])\r\n",
    "gs_best_knn.fit(X_train,Y_train)\r\n",
    "print(f'test score : {gs_best_knn.score(X_test, Y_test)}')\r\n",
    "knn_Y_predict = gs_best_knn.predict(X_test)\r\n",
    "print('Recall :{}'.format(recall_score(Y_test,knn_Y_predict)))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best parameters: {'metric': 'euclidean', 'n_neighbors': 20, 'weights': 'distance'}\n",
      "Best cross-validation score: 0.75\n",
      "test score : 0.6554621848739496\n",
      "Recall :0.7631578947368421\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Best parameters from the grid search are- metric: euclidean, n_neighbors: 20, weights: distance. The Cross-validation score of this model is 0.75. Test Score of the model with the best parameters is 0.66 and the Recall score is 0.76. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "source": [
    "# LOGISTIC REGRESSION\r\n",
    "\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "import warnings\r\n",
    "\r\n",
    "grid_params= {'penalty': ['l1', 'l2'], 'C': [0.05, 0.1, 1, 10, 100, 500], 'solver': ['lbfgs','saga','sag'] }\r\n",
    "\r\n",
    "Logreg = GridSearchCV(LogisticRegression(), param_grid = grid_params, cv = 5, n_jobs=-1, scoring= 'recall')\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "best_logreg = Logreg.fit(X_train, Y_train)\r\n",
    "print(\"Best parameters: {}\".format(Logreg.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(Logreg.best_score_))\r\n",
    "\r\n",
    "logL2= LogisticRegression(penalty= Logreg.best_params_['penalty'], C= Logreg.best_params_['C'], solver= Logreg.best_params_['solver'])\r\n",
    "logL2.fit(X_train, Y_train)\r\n",
    "print('train_score_l2 : {}'.format(logL2.score(X_train, Y_train)))\r\n",
    "print('test_score_l2 : {}'.format(logL2.score(X_test, Y_test)))\r\n",
    "\r\n",
    "Logreg_Y_predict = logL2.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,Logreg_Y_predict)))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'C': 0.05, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best cross-validation score: 0.81\n",
      "train_score_l2 : 0.6539540100953449\n",
      "test_score_l2 : 0.6621848739495798\n",
      "Recall :0.8129\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters for Logistic Regression model is C= 0.05, penalty= l1, solver= saga. When the model is run with these parameters, we get a train score of 0.66 and Recall score of 0.81. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "source": [
    "# LINEAR SVC\r\n",
    "\r\n",
    "from sklearn.svm import LinearSVC\r\n",
    "\r\n",
    "grid_params= {'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100, 1000], 'random_state': [0] }\r\n",
    "\r\n",
    "clf_lsvc = GridSearchCV(LinearSVC(), param_grid = grid_params, cv = 5, n_jobs=-1, scoring= 'recall')\r\n",
    "\r\n",
    "best_lsvc = clf_lsvc.fit(X_train, Y_train)\r\n",
    "print(\"Best parameters: {}\".format(clf_lsvc.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(clf_lsvc.best_score_))\r\n",
    "\r\n",
    "clf_ls= LinearSVC(penalty= clf_lsvc.best_params_['penalty'], C= clf_lsvc.best_params_['C'], random_state= clf_lsvc.best_params_['random_state'])\r\n",
    "clf_ls.fit(X_train, Y_train)\r\n",
    "print('train_score_l2 : {}'.format(clf_ls.score(X_train, Y_train)))\r\n",
    "print('test_score_l2 : {}'.format(clf_ls.score(X_test, Y_test)))\r\n",
    "\r\n",
    "clf_ls_Ypredict = clf_ls.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,clf_ls_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'C': 0.01, 'penalty': 'l2', 'random_state': 0}\n",
      "Best cross-validation score: 0.75\n",
      "train_score_l2 : 0.6679753224901851\n",
      "test_score_l2 : 0.66890756302521\n",
      "Recall :0.7661\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "source": [
    "# DECISION TREE\r\n",
    "\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "\r\n",
    "grid_params ={'criterion': ['gini', 'entropy'], 'max_depth': [2,5,7], 'min_samples_leaf':[5,10,15],  'random_state': [0]}\r\n",
    "\r\n",
    "clf_tree = GridSearchCV(DecisionTreeClassifier(), param_grid = grid_params, cv = 5, n_jobs=-1, scoring= 'recall')\r\n",
    "\r\n",
    "best_tree = clf_tree.fit(X_train, Y_train)\r\n",
    "print(\"Best parameters: {}\".format(clf_tree.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(clf_tree.best_score_))\r\n",
    "\r\n",
    "tree = DecisionTreeClassifier(**clf_tree.best_params_)\r\n",
    "tree.fit(X_train, Y_train)\r\n",
    "\r\n",
    "print(\"Train score: {:.4f}\".format(tree.score(X_train, Y_train)))\r\n",
    "print(\"Test score: {:.4f}\".format(tree.score(X_test,Y_test)))\r\n",
    "\r\n",
    "tree_Ypredict = tree.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,tree_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 5, 'random_state': 0}\n",
      "Best cross-validation score: 0.77\n",
      "Train score: 0.6854\n",
      "Test score: 0.6437\n",
      "Recall :0.6930\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters obtained from the Grid Search are: criterion= 'entropy', max_depth= 5, min_samples_leaf= 5. The cross-validation score is 0.77 The decicion tree model run with these parameters provides a train score of 0.64 and Recall score of 0.69."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "source": [
    "# SVC KERNEL\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.metrics import recall_score, precision_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "source": [
    "# kernel= poly\r\n",
    "\r\n",
    "grid_params ={'kernel': ['poly'], 'C': [0.1,0.3], 'degree':[2,3,4],  'gamma': [0.1,0.3]}\r\n",
    "\r\n",
    "clf_poly = GridSearchCV(SVC(random_state=0), param_grid = grid_params, cv = 5, scoring= 'recall',verbose=True)\r\n",
    "\r\n",
    "clf_poly.fit(X_train, Y_train)\r\n",
    "\r\n",
    "print(\"Best parameters: {}\".format(clf_poly.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(clf_poly.best_score_))\r\n",
    "\r\n",
    "clf_bestpoly = SVC(**clf_poly.best_params_)\r\n",
    "clf_bestpoly.fit(X_train,Y_train)\r\n",
    "\r\n",
    "print('train_score_l2 : {}'.format(clf_bestpoly.score(X_train, Y_train)))\r\n",
    "print('test_score_l2 : {}'.format(clf_bestpoly.score(X_test, Y_test)))\r\n",
    "\r\n",
    "clf_bestpoly_Ypredict = clf_bestpoly.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,clf_bestpoly_Ypredict)))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters: {'C': 0.1, 'degree': 4, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "Best cross-validation score: 0.85\n",
      "train_score_l2 : 0.6870443073471677\n",
      "test_score_l2 : 0.6487394957983194\n",
      "Recall :0.8363\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We notice that SVC With kernel 'poly', degree '4' and C = 0.1 and gamma 0.1 provides the test score 0.65 and a Recall score of 0.84. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "source": [
    "#kernel= rbf\r\n",
    "grid_params ={'kernel': ['rbf'], 'C': [0.1,0.3], 'degree':[2,3,4],  'gamma': [0.1,0.3]}\r\n",
    "\r\n",
    "clf_rbf = GridSearchCV(SVC(random_state=0), param_grid = grid_params, cv = 5, scoring= 'recall',verbose=True)\r\n",
    "\r\n",
    "clf_rbf.fit(X_train, Y_train)\r\n",
    "\r\n",
    "print(\"Best parameters: {}\".format(clf_rbf.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(clf_rbf.best_score_))\r\n",
    "\r\n",
    "clf_bestrbf = SVC(**clf_rbf.best_params_)\r\n",
    "clf_bestrbf.fit(X_train,Y_train)\r\n",
    "\r\n",
    "print('train_score_l2 : {}'.format(clf_bestrbf.score(X_train, Y_train)))\r\n",
    "print('test_score_l2 : {}'.format(clf_bestrbf.score(X_test, Y_test)))\r\n",
    "\r\n",
    "clf_bestrbf_Ypredict = clf_bestrbf.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,clf_bestrbf_Ypredict)))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters: {'C': 0.1, 'degree': 2, 'gamma': 0.3, 'kernel': 'rbf'}\n",
      "Best cross-validation score: 0.93\n",
      "train_score_l2 : 0.683679192372406\n",
      "test_score_l2 : 0.6403361344537815\n",
      "Recall :0.8772\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "SVC With kernel 'rbf', degree '2' and C = 0.1 and gamma 0.3 provides Test score of 0.64 and Recall score of 0.88."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "source": [
    "#kernel= linear\r\n",
    "grid_params ={'kernel': ['linear'], 'C': [0.1,0.3], 'degree':[2,3,4],  'gamma': [0.1,0.3]}\r\n",
    "\r\n",
    "clf_lin = GridSearchCV(SVC(random_state=0), param_grid = grid_params, cv = 5, scoring= 'recall',verbose=True)\r\n",
    "\r\n",
    "clf_lin.fit(X_train, Y_train)\r\n",
    "\r\n",
    "print(\"Best parameters: {}\".format(clf_lin.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(clf_lin.best_score_))\r\n",
    "\r\n",
    "clf_bestlin = SVC(**clf_lin.best_params_)\r\n",
    "clf_bestlin.fit(X_train,Y_train)\r\n",
    "\r\n",
    "print('train_score_l2 : {}'.format(clf_bestlin.score(X_train, Y_train)))\r\n",
    "print('test_score_l2 : {}'.format(clf_bestlin.score(X_test, Y_test)))\r\n",
    "\r\n",
    "clf_bestlin_Ypredict = clf_bestlin.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,clf_bestlin_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters: {'C': 0.1, 'degree': 2, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Best cross-validation score: 0.72\n",
      "train_score_l2 : 0.6741446999439148\n",
      "test_score_l2 : 0.6789915966386555\n",
      "Recall :0.7456\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We notice that SVC With kernel 'linear', degree '2' and C = 0.1 and gamma 0.1 provides a test score of 0.68 and a Recall score 0.75. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "source": [
    "# RANDOM FOREST\r\n",
    "\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.metrics import precision_recall_curve\r\n",
    "\r\n",
    "grid_params ={'n_estimators': [100, 200, 300], 'max_depth': [4,6,7], 'min_samples_leaf':[5,7,10],  'random_state': [0], 'max_features' : [5,7]}\r\n",
    "\r\n",
    "rf = GridSearchCV(RandomForestClassifier(), param_grid = grid_params, cv = 5, n_jobs=-1, scoring= 'recall')\r\n",
    "\r\n",
    "rf.fit(X_train, Y_train)\r\n",
    "print(\"Best parameters: {}\".format(rf.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(rf.best_score_))\r\n",
    "\r\n",
    "RForest= RandomForestClassifier(**rf.best_params_)\r\n",
    "RForest.fit(X_train, Y_train)\r\n",
    "\r\n",
    "print(\"Train score: {:.4f}\".format(RForest.score(X_train, Y_train)))\r\n",
    "print(\"Test score: {:.4f}\".format(RForest.score(X_test, Y_test)))\r\n",
    "\r\n",
    "RForest_Ypredict = RForest.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test, RForest_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'max_depth': 4, 'max_features': 5, 'min_samples_leaf': 10, 'n_estimators': 100, 'random_state': 0}\n",
      "Best cross-validation score: 0.84\n",
      "Train score: 0.6988\n",
      "Test score: 0.6975\n",
      "Recall :0.8801\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters for the model after Grid Search with cross-validation is max_depth= 4, max_features= 5, min_samples_leaf= 10, n_estimators= 100. With these parameters, we run the Random Forest , we get a test score of 0.70 and a recall score of 0.88."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next I will be building models with ensemble methods. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VOTING CLASSIFIER"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I am developing voting classifiers with Kernel SVC (poly), Decision tree and Logistic Regression. I am using the best parameters found in the Grid search for their respective models as the parameters used for these classifiers. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "source": [
    "# VOTING CLASSIFIERS \r\n",
    "from sklearn.ensemble import VotingClassifier\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "source": [
    "# Hard Voting\r\n",
    "\r\n",
    "svc= SVC(random_state=0, kernel= 'poly', degree= 4, C= 0.1, gamma= 0.1, probability= True)\r\n",
    "svc.fit(X_train, Y_train)\r\n",
    "\r\n",
    "tree_cls= DecisionTreeClassifier(criterion='entropy', max_depth= 5, min_samples_leaf=5,random_state= 0)\r\n",
    "tree_cls.fit(X_train, Y_train)\r\n",
    "\r\n",
    "log_cls= LogisticRegression(solver= 'saga', penalty= 'l1', C=0.05, max_iter= 1000, multi_class= 'auto',random_state= 0)\r\n",
    "log_cls.fit(X_train, Y_train)\r\n",
    "\r\n",
    "# Voting= hard\r\n",
    "hardVoting_cls= VotingClassifier(estimators= [('svc', svc), ('tree', tree_cls), ('log', log_cls)], voting= 'hard')\r\n",
    "hardVoting_cls.fit(X_train,Y_train)\r\n",
    "hardVoting_cls.score(X_train,Y_train)\r\n",
    "\r\n",
    "y_pred_voting= hardVoting_cls.predict(X_test)\r\n",
    "print('Train score: {}'.format(hardVoting_cls.score(X_train,Y_train)))\r\n",
    "print('Test score: {}'.format(hardVoting_cls.score(X_test,Y_test)))\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test, y_pred_voting)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train score: 0.6859226023555804\n",
      "Test score: 0.6621848739495798\n",
      "Recall :0.8099\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Voting Classifier developed with voting method set to \"hard\" provides a recall score of 0.81 and Test score of 0.66."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "source": [
    "# Soft Voting\r\n",
    "\r\n",
    "voting_cls_soft= VotingClassifier(estimators= [('svc', svc), ('tree', tree_cls), ('log', log_cls)], voting= 'soft')\r\n",
    "voting_cls_soft.fit(X_train,Y_train)\r\n",
    "voting_cls_soft.score(X_train,Y_train)\r\n",
    "\r\n",
    "y_pred_voting= voting_cls_soft.predict(X_test)\r\n",
    "print('Train score: {}'.format(voting_cls_soft.score(X_train,Y_train)))\r\n",
    "print('Test score: {}'.format(voting_cls_soft.score(X_test,Y_test)))\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test, y_pred_voting)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train score: 0.7083567021873247\n",
      "Test score: 0.6941176470588235\n",
      "Recall :0.7544\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Voting Classifier developed with voting method set to \"soft\" provides a Recall score of 0.75 and Test score of 0.69. \n",
    "Soft Voting Classifier has better Test score than Hard Voting Classifier. However we care more about the Recall value than Accuracy. Recall score Hard VC(0.81) is better than Soft VC. Thus, Hard Voting Classifier is doing a better job than Soft Voting Classifier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "source": [
    "# BAGGING\r\n",
    "\r\n",
    "# bagging on Decision tree with gridsearch\r\n",
    "from sklearn.ensemble import BaggingClassifier\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from  sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "tree_bag= DecisionTreeClassifier( criterion= 'entropy', min_samples_leaf= 5, max_depth= 5, random_state= 0)\r\n",
    "grid_params_bag= {'max_features': [5, 10, 15], 'n_estimators': [100, 200, 300], 'max_samples': [0.1, 0.5]}\r\n",
    "\r\n",
    "grid_bag= GridSearchCV(\r\n",
    "    BaggingClassifier(tree_bag, random_state=0), param_grid= grid_params_bag, cv= 5, scoring= 'recall')\r\n",
    "best_tree_bag= grid_bag.fit(X_train, Y_train)\r\n",
    "\r\n",
    "print('Best parameters: {}'.format(grid_bag.best_params_))\r\n",
    "print('Best CV score:{}'.format(grid_bag.best_score_))\r\n",
    "\r\n",
    "tree_bagging= BaggingClassifier(tree_bag,**grid_bag.best_params_, random_state=0, bootstrap=True)\r\n",
    "tree_bagging.fit(X_train, Y_train)\r\n",
    "print('Train_score : {}'.format(tree_bagging.score(X_train, Y_train)))\r\n",
    "print('Test_score : {}'.format(tree_bagging.score(X_test, Y_test)))\r\n",
    "\r\n",
    "tree_bag_Ypredict = tree_bagging.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,tree_bag_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'max_features': 5, 'max_samples': 0.1, 'n_estimators': 300}\n",
      "Best CV score:0.9229862072501666\n",
      "Train_score : 0.641054402692092\n",
      "Test_score : 0.6235294117647059\n",
      "Recall :0.9298\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grid Search on Bagging with Decision Tree gives the following best parameters: max_features: 5, max_samples: 0.1, n_estimators: 300. Test score is 0.62 and Recall score is 0.93. This is the best Recall so far"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "source": [
    "# Bagging on KNN with Grid search\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.metrics import recall_score, precision_score\r\n",
    "\r\n",
    "knn = KNeighborsClassifier(metric= 'euclidean', n_neighbors= 20, weights= 'distance')\r\n",
    "\r\n",
    "grid_params_bag ={'max_features': [5, 7, 10], 'n_estimators': [100, 200, 500], 'max_samples': [0.1, 0.5]}\r\n",
    "\r\n",
    "grid_knn = GridSearchCV(BaggingClassifier(knn, random_state=0), param_grid = grid_params_bag, cv = 5, scoring= 'recall')\r\n",
    "\r\n",
    "grid_best_ksvc = grid_knn.fit(X_train, Y_train)\r\n",
    "print(\"Best parameters: {}\".format(grid_knn.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_knn.best_score_))\r\n",
    "\r\n",
    "knn_bag= BaggingClassifier(knn, **grid_knn.best_params_, random_state=0)\r\n",
    "knn_bag.fit(X_train, Y_train)\r\n",
    "print('train_score_l2 : {}'.format(knn_bag.score(X_train, Y_train)))\r\n",
    "print('test_score_l2 : {}'.format(knn_bag.score(X_test, Y_test)))\r\n",
    "\r\n",
    "knn_bag_Ypredict = knn_bag.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,knn_bag_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'max_features': 5, 'max_samples': 0.1, 'n_estimators': 500}\n",
      "Best cross-validation score: 0.92\n",
      "train_score_l2 : 0.6920919798093101\n",
      "test_score_l2 : 0.6302521008403361\n",
      "Recall :0.9269\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grid Search on Bagging with KNN Classifier gives the following best parameters: max_features: 5, max_samples: 0.1, n_estimators: 500. Test score is 0.63 and Recall score is 0.927."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "source": [
    "# PASTING\r\n",
    "\r\n",
    "# pasting with decision tree, gridsearch\r\n",
    "\r\n",
    "grid_tree_paste= GridSearchCV(\r\n",
    "    BaggingClassifier(tree_bag, random_state=0, bootstrap=False), param_grid= grid_params_bag, cv= 5, scoring= 'recall')\r\n",
    "grid_tree_paste.fit(X_train, Y_train)\r\n",
    "\r\n",
    "print('Best parameters: {}'.format(grid_tree_paste.best_params_))\r\n",
    "print('Best CV score:{}'.format(grid_tree_paste.best_score_))\r\n",
    "\r\n",
    "tree_paste= BaggingClassifier(tree_bag,**grid_tree_paste.best_params_, random_state=0, bootstrap= False)\r\n",
    "tree_paste.fit(X_train, Y_train)\r\n",
    "print('Train_score : {}'.format(tree_paste.score(X_train, Y_train)))\r\n",
    "print('Test_score : {}'.format(tree_paste.score(X_test, Y_test)))\r\n",
    "\r\n",
    "tree_paste_Ypredict = tree_paste.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,tree_paste_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'max_features': 5, 'max_samples': 0.1, 'n_estimators': 500}\n",
      "Best CV score:0.9280572219658515\n",
      "Train_score : 0.6421761076836792\n",
      "Test_score : 0.6134453781512605\n",
      "Recall :0.9035\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grid Search on Pasting with Decision tree gives the following best parameters: max_features: 5, max_samples: 0.1, n_estimators: 500. Test score is 0.61 and Recall score is 0.90."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "source": [
    "# pasting on KNN, grid search\r\n",
    "knn = KNeighborsClassifier(metric= 'euclidean', n_neighbors= 5, weights= 'distance')\r\n",
    "\r\n",
    "grid_params_bag ={'max_features': [5, 7, 10], 'n_estimators': [100, 200, 300], 'max_samples': [0.1, 0.5]}\r\n",
    "grid_knn_paste = GridSearchCV(BaggingClassifier(knn, random_state=0, bootstrap=False), param_grid = grid_params_bag, cv = 5, scoring= 'recall')\r\n",
    "\r\n",
    "grid_knn_paste.fit(X_train, Y_train)\r\n",
    "print(\"Best parameters: {}\".format(grid_knn_paste.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_knn_paste.best_score_))\r\n",
    "\r\n",
    "knn_paste= BaggingClassifier(knn,**grid_knn_paste.best_params_, random_state=0, bootstrap= False)\r\n",
    "knn_paste.fit(X_train, Y_train)\r\n",
    "print('Train_score : {}'.format(knn_paste.score(X_train, Y_train)))\r\n",
    "print('Test_score : {}'.format(knn_paste.score(X_test, Y_test)))\r\n",
    "knn_paste_Ypredict = knn_paste.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test, knn_paste_Ypredict)))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'max_features': 5, 'max_samples': 0.1, 'n_estimators': 300}\n",
      "Best cross-validation score: 0.91\n",
      "Train_score : 0.699383062254627\n",
      "Test_score : 0.6487394957983194\n",
      "Recall :0.9035\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grid Search on Pasting with KNN Classifier gives the following best parameters: max_features: 5, max_samples: 0.1, n_estimators: 300. Test score is 0.65 and Recall score is 0.90."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "source": [
    "# ADABOOST\r\n",
    "\r\n",
    "# Adaboost Classifier with Decision Tree\r\n",
    "\r\n",
    "from sklearn.ensemble import AdaBoostClassifier\r\n",
    "tree_bag= DecisionTreeClassifier( criterion= 'entropy', min_samples_leaf= 10, max_depth= 7, random_state= 0)\r\n",
    "\r\n",
    "\r\n",
    "grid_params_ada ={'learning_rate': [0.5, 1, 1.5], 'n_estimators': [100, 200, 300], 'algorithm': ['SAMME', 'SAMME.R']}\r\n",
    "\r\n",
    "ada_tree = GridSearchCV(\r\n",
    "    AdaBoostClassifier(tree_bag, random_state=0), param_grid= grid_params_ada, cv=5, scoring= 'recall')\r\n",
    "ada_tree.fit(X_train, Y_train)\r\n",
    "\r\n",
    "print(\"Best parameters: {}\".format(ada_tree.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(ada_tree.best_score_))\r\n",
    "\r\n",
    "tree_ada= AdaBoostClassifier(tree_bag,**ada_tree.best_params_, random_state=0)\r\n",
    "tree_ada.fit(X_train, Y_train)\r\n",
    "print('Train_score : {}'.format(tree_ada.score(X_train, Y_train)))\r\n",
    "print('Test_score : {}'.format(tree_ada.score(X_test, Y_test)))\r\n",
    "tree_ada_Ypredict= tree_ada.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test, tree_ada_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'algorithm': 'SAMME', 'learning_rate': 0.5, 'n_estimators': 300}\n",
      "Best cross-validation score: 0.73\n",
      "Train_score : 1.0\n",
      "Test_score : 0.6773109243697479\n",
      "Recall :0.7398\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grid Search on Adaboost with Decision tree gives the following best parameters: algorithm: 'SAMME', learning_rate: 0.5, n_estimators: 300. <br> Test score is 0.67 and Recall score is 0.74. There seems to be overfitting with this model since train score is 1.0"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "rf = RandomForestClassifier(max_depth= 6, max_features= 2, min_samples_leaf= 10, n_estimators= 10, random_state= 0)\r\n",
    "\r\n",
    "grid_params ={'learning_rate': [0.5, 0.7,0.9], 'n_estimators': [50,100, 150], 'algorithm': ['SAMME']}\r\n",
    "\r\n",
    "grid_ada_rf = GridSearchCV(\r\n",
    "    AdaBoostClassifier(rf, random_state=0), param_grid= grid_params, cv=5, scoring= 'recall',return_train_score=True)\r\n",
    "grid_ada_rf.fit(X_train, Y_train)\r\n",
    "\r\n",
    "print(\"Best parameters: {}\".format(grid_ada_rf.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_ada_rf.best_score_))\r\n",
    "\r\n",
    "rf_ada= AdaBoostClassifier(rf,**grid_ada_rf.best_params_, random_state=0)\r\n",
    "rf_ada.fit(X_train, Y_train)\r\n",
    "print('Train_score : {}'.format(rf_ada.score(X_train, Y_train)))\r\n",
    "print('Test_score : {}'.format(rf_ada.score(X_test, Y_test)))\r\n",
    "\r\n",
    "rf_ada_Ypredict= rf_ada.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test, rf_ada_Ypredict)))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'algorithm': 'SAMME', 'learning_rate': 0.5, 'n_estimators': 50}\n",
      "Best cross-validation score: 0.73\n",
      "Train_score : 0.9584969153112731\n",
      "Test_score : 0.6487394957983194\n",
      "Recall :0.7135\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grid Search on Adaboost with Random forest Classifier gives the following best parameters: algorithm: SAMME, learning_rate: 0.5, n_estimators: 50. Test score is 0.65 and Recall score is 0.71. There seems to be some amount of overfitting since train and test scores have a huge gap between them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Boosting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "source": [
    "from  sklearn.ensemble import GradientBoostingClassifier\r\n",
    "grid_params= {'max_features': [2, 5, 10, 20], 'n_estimators': [100, 200, 500], 'learning_rate': [0.1, 0.25, 0.5, 0.75, 1]}\r\n",
    "\r\n",
    "grid_boost= GridSearchCV(\r\n",
    "    GradientBoostingClassifier(random_state=0), param_grid= grid_params, cv= 5, scoring='recall' )\r\n",
    "grid_boost.fit(X_train, Y_train)\r\n",
    "\r\n",
    "print('Best parameters: {}'.format(grid_boost.best_params_))\r\n",
    "print('Best CV score:{}'.format(grid_boost.best_score_))\r\n",
    "\r\n",
    "gboost= GradientBoostingClassifier(**grid_boost.best_params_, random_state=0)\r\n",
    "gboost.fit(X_train, Y_train)\r\n",
    "print('Train_score : {}'.format(gboost.score(X_train, Y_train)))\r\n",
    "print('Test_score : {}'.format(gboost.score(X_test, Y_test)))\r\n",
    "\r\n",
    "gboost_Ypredict= gboost.predict(X_test)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test, gboost_Ypredict)))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'learning_rate': 0.1, 'max_features': 2, 'n_estimators': 100}\n",
      "Best CV score:0.7618571501820233\n",
      "Train_score : 0.7291082445316882\n",
      "Test_score : 0.7025210084033613\n",
      "Recall :0.7924\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grid Search on Gradient boosting gives the following best parameters: max_features: 2, learning_rate: 0.1, n_estimators: 100. Test score is 0.70 and Recall score is 0.79. This is a decent model with a good Test score and a good Recall score."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I am using Principal Component Analysis for Dimension Reduction of the dataset. After that I run all the original models on the new reduced dataset. Both X_train ad X_test are reduced to retain 95% of the information in the dataset. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "source": [
    "# Creating an empty DataFrame for the model results.\r\n",
    "\r\n",
    "pca_results_df = pd.DataFrame(columns=['model','test_score','recall_score'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "source": [
    "from sklearn.decomposition import PCA\r\n",
    "\r\n",
    "pca= PCA(n_components= 0.95)\r\n",
    "X_train_reduced= pca.fit_transform(X_train)\r\n",
    "X_test_reduced= pca.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "source": [
    "# KNN\r\n",
    "from sklearn.neighbors import KNeighborsClassifier \r\n",
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "grid_params= {'n_neighbors': [3,4,5,8,10,15,20],'weights':['distance'],'metric':['euclidean']}\r\n",
    "\r\n",
    "pca_gs_knn= GridSearchCV(KNeighborsClassifier(), grid_params, cv= 5 ,scoring = 'recall',return_train_score=True)\r\n",
    "pca_gs_results= pca_gs_knn.fit(X_train, Y_train)\r\n",
    "print(\"Best parameters: {}\".format(pca_gs_knn.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(pca_gs_knn.best_score_))\r\n",
    "\r\n",
    "pca_gs_best_knn = KNeighborsClassifier(n_neighbors = pca_gs_results.best_params_['n_neighbors'],metric=pca_gs_results.best_params_['metric'],weights=pca_gs_results.best_params_['weights'])\r\n",
    "pca_gs_best_knn.fit(X_train_reduced,Y_train)\r\n",
    "print(f'test score : {pca_gs_best_knn.score(X_test_reduced, Y_test)}')\r\n",
    "knn_Y_predict = pca_gs_best_knn.predict(X_test_reduced)\r\n",
    "print('Recall :{}'.format(recall_score(Y_test,knn_Y_predict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'metric': 'euclidean', 'n_neighbors': 20, 'weights': 'distance'}\n",
      "Best cross-validation score: 0.75\n",
      "test score : 0.6605042016806723\n",
      "Recall :0.7573099415204678\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters for the model after Grid Search with cross-validation is metric: euclidean, n_neighbors: 20, weights: distance. With these parameters, we run the KNN Classifier , we get a test score of 0.66 and a recall score of 0.76."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "source": [
    "# Logistic regression\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "import warnings\r\n",
    "\r\n",
    "grid_params= {'penalty': ['l1', 'l2'], 'C': [0.05, 0.1, 1, 10, 100, 500], 'solver': ['lbfgs','saga'] }\r\n",
    "\r\n",
    "pca_Logreg = GridSearchCV(LogisticRegression(), param_grid = grid_params, cv = 5, n_jobs=-1, scoring= 'recall')\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "\r\n",
    "pca_best_logreg = pca_Logreg.fit(X_train_reduced, Y_train)\r\n",
    "print(\"Best parameters: {}\".format(pca_Logreg.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(pca_Logreg.best_score_))\r\n",
    "\r\n",
    "pca_logL2= LogisticRegression(**pca_Logreg.best_params_)\r\n",
    "pca_logL2.fit(X_train_reduced, Y_train)\r\n",
    "print('train_score_l2 : {}'.format(pca_logL2.score(X_train_reduced, Y_train)))\r\n",
    "print('test_score_l2 : {}'.format(pca_logL2.score(X_test_reduced, Y_test)))\r\n",
    "\r\n",
    "Logreg_Y_predict = pca_logL2.predict(X_test_reduced)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,Logreg_Y_predict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'C': 0.05, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best cross-validation score: 0.81\n",
      "train_score_l2 : 0.6511497476163769\n",
      "test_score_l2 : 0.6252100840336134\n",
      "Recall :0.7807\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters for the model after Grid Search with cross-validation is C: 0.05, penalty: l1, solver: saga. The KNN Classifier provides a test score of 0.63 and a recall score of 0.78."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "source": [
    "# Decision tree\r\n",
    "\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "\r\n",
    "grid_params ={'criterion': ['gini', 'entropy'], 'max_depth': [2,5,7], 'min_samples_leaf':[5,10,15],  'random_state': [0]}\r\n",
    "\r\n",
    "pca_clf_tree = GridSearchCV(DecisionTreeClassifier(), param_grid = grid_params, cv = 5, n_jobs=-1, scoring= 'recall')\r\n",
    "\r\n",
    "pca_best_tree = pca_clf_tree.fit(X_train_reduced, Y_train)\r\n",
    "print(\"Best parameters: {}\".format(pca_clf_tree.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(pca_clf_tree.best_score_))\r\n",
    "\r\n",
    "pca_tree = DecisionTreeClassifier(**pca_clf_tree.best_params_)\r\n",
    "pca_tree.fit(X_train_reduced, Y_train)\r\n",
    "\r\n",
    "print(\"Train score: {:.4f}\".format(pca_tree.score(X_train_reduced, Y_train)))\r\n",
    "print(\"Test score: {:.4f}\".format(pca_tree.score(X_test_reduced,Y_test)))\r\n",
    "\r\n",
    "tree_Ypredict = pca_tree.predict(X_test_reduced)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,tree_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5, 'random_state': 0}\n",
      "Best cross-validation score: 0.74\n",
      "Train score: 0.6125\n",
      "Test score: 0.6050\n",
      "Recall :0.8421\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters for the model after Grid Search with cross-validation is criterion: gini, max_depth: 2, min_samples_leaf: 5. With these parameters, we get a test score of 0.60 and a recall score of 0.84."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "source": [
    "# LinearSVC\r\n",
    "\r\n",
    "from sklearn.svm import LinearSVC\r\n",
    "\r\n",
    "grid_params= {'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100, 1000], 'random_state': [0] }\r\n",
    "\r\n",
    "pca_lsvc = GridSearchCV(LinearSVC(), param_grid = grid_params, cv = 5, n_jobs=-1, scoring= 'recall')\r\n",
    "\r\n",
    "best_lsvc = pca_lsvc.fit(X_train_reduced, Y_train)\r\n",
    "print(\"Best parameters: {}\".format(pca_lsvc.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(pca_lsvc.best_score_))\r\n",
    "\r\n",
    "pca_ls= LinearSVC(**pca_lsvc.best_params_)\r\n",
    "pca_ls.fit(X_train_reduced, Y_train)\r\n",
    "print('train_score : {}'.format(pca_ls.score(X_train_reduced, Y_train)))\r\n",
    "print('test_score : {}'.format(pca_ls.score(X_test_reduced, Y_test)))\r\n",
    "\r\n",
    "pca_ls_Ypredict = pca_ls.predict(X_test_reduced)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,pca_ls_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'C': 0.01, 'penalty': 'l2', 'random_state': 0}\n",
      "Best cross-validation score: 0.74\n",
      "train_score : 0.6679753224901851\n",
      "test_score : 0.6722689075630253\n",
      "Recall :0.7690\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters for the model after Grid Search with cross-validation is C': 0.01, 'penalty': 'l2'. With these parameters, the Linear SVC model provides a test score of 0.67 and a recall score of 0.77."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "source": [
    "# KERNEL SVC\r\n",
    "# kernel= poly\r\n",
    "\r\n",
    "grid_params_poly ={'kernel': ['poly'], 'C': [0.1,0.3], 'degree':[2,3,4],  'gamma': [0.1,0.3]}\r\n",
    "\r\n",
    "pca_poly = GridSearchCV(SVC(random_state=0), param_grid = grid_params_poly, cv = 5, scoring= 'recall',verbose=True)\r\n",
    "\r\n",
    "pca_poly.fit(X_train_reduced, Y_train)\r\n",
    "\r\n",
    "print(\"Best parameters: {}\".format(pca_poly.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(pca_poly.best_score_))\r\n",
    "\r\n",
    "pca_bestpoly = SVC(**pca_poly.best_params_)\r\n",
    "pca_bestpoly.fit(X_train_reduced,Y_train)\r\n",
    "\r\n",
    "print('train_score : {}'.format(pca_bestpoly.score(X_train_reduced, Y_train)))\r\n",
    "print('test_score : {}'.format(pca_bestpoly.score(X_test_reduced, Y_test)))\r\n",
    "\r\n",
    "pca_bestpoly_Ypredict = pca_bestpoly.predict(X_test_reduced)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,pca_bestpoly_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters: {'C': 0.1, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}\n",
      "Best cross-validation score: 1.00\n",
      "train_score : 0.5535614133482895\n",
      "test_score : 0.5747899159663865\n",
      "Recall :1.0000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters for the model after Grid Search with cross-validation is C: 0.1, degree: 2, gamma: 0.1, kernel: poly. With these parameters we get a test score of 0.57 and a recall score of 1. The model seems to be underfitting significantly with a train score of 0.55."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "source": [
    "#kernel= rbf\r\n",
    "grid_params ={'kernel': ['rbf'], 'C': [0.1,0.3], 'degree':[2,3,4],  'gamma': [0.1,0.3]}\r\n",
    "\r\n",
    "pca_rbf = GridSearchCV(SVC(random_state=0), param_grid = grid_params, cv = 5, scoring= 'recall',verbose=False)\r\n",
    "pca_rbf.fit(X_train_reduced, Y_train)\r\n",
    "print(\"Best parameters: {}\".format(pca_rbf.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(pca_rbf.best_score_))\r\n",
    "\r\n",
    "pca_bestrbf = SVC(**pca_rbf.best_params_)\r\n",
    "pca_bestrbf.fit(X_train_reduced, Y_train)\r\n",
    "\r\n",
    "print('train_score : {}'.format(pca_bestrbf.score(X_train_reduced, Y_train)))\r\n",
    "print('test_score : {}'.format(pca_bestrbf.score(X_test_reduced, Y_test)))\r\n",
    "\r\n",
    "pca_bestrbf_Ypredict = pca_bestrbf.predict(X_test_reduced)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test,pca_bestrbf_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'C': 0.1, 'degree': 2, 'gamma': 0.3, 'kernel': 'rbf'}\n",
      "Best cross-validation score: 0.89\n",
      "train_score : 0.6887268648345485\n",
      "test_score : 0.6470588235294118\n",
      "Recall :0.8421\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters for the model after Grid Search with cross-validation is C: 0.1, degree: 2, gamma: 0.3, kernel: rbf. With these parameters, we get a test score of 0.65 and a recall score of 0.84."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "source": [
    "#kernel= linear\r\n",
    "grid_params ={'kernel': ['linear'], 'C': [0.1,0.3], 'degree':[2,3,4],  'gamma': [0.1,0.3]}\r\n",
    "\r\n",
    "pca_lin = GridSearchCV(SVC(random_state=0), param_grid = grid_params, cv = 5, scoring= 'recall',verbose=True)\r\n",
    "\r\n",
    "pca_lin.fit(X_train_reduced, Y_train)\r\n",
    "\r\n",
    "print(\"Best parameters: {}\".format(pca_lin.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(pca_lin.best_score_))\r\n",
    "\r\n",
    "pca_bestlin = SVC(**pca_lin.best_params_)\r\n",
    "pca_bestlin.fit(X_train_reduced,Y_train)\r\n",
    "\r\n",
    "print('train_score_l2 : {}'.format(pca_bestlin.score(X_train_reduced, Y_train)))\r\n",
    "print('test_score_l2 : {}'.format(pca_bestlin.score(X_test_reduced, Y_test)))\r\n",
    "\r\n",
    "pca_bestlin_Ypredict = pca_bestlin.predict(X_test_reduced)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test, pca_bestlin_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters: {'C': 0.3, 'degree': 2, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Best cross-validation score: 0.72\n",
      "train_score_l2 : 0.6713404374649468\n",
      "test_score_l2 : 0.6789915966386555\n",
      "Recall :0.7573\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters for the model after Grid Search with cross-validation is C: 0.1, degree: 2, gamma: 0.1, kernel: linear. With these parameters, we get a test score of 0.68 and a recall score of 0.76."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "source": [
    "# RANDOM FOREST\r\n",
    "\r\n",
    "grid_params ={'n_estimators': [100, 200, 500], 'max_depth': [2,4,6], 'min_samples_leaf':[5,10],  'random_state': [0], 'max_features' : [2]}\r\n",
    "\r\n",
    "pca_rf = GridSearchCV(RandomForestClassifier(), param_grid = grid_params, cv = 5, n_jobs=-1, scoring= 'recall')\r\n",
    "\r\n",
    "pca_rf.fit(X_train_reduced, Y_train)\r\n",
    "print(\"Best parameters: {}\".format(pca_rf.best_params_))\r\n",
    "print(\"Best cross-validation score: {:.2f}\".format(pca_rf.best_score_))\r\n",
    "\r\n",
    "pca_RForest= RandomForestClassifier(**pca_rf.best_params_)\r\n",
    "pca_RForest.fit(X_train_reduced, Y_train)\r\n",
    "\r\n",
    "print(\"Train score: {:.4f}\".format(pca_RForest.score(X_train_reduced, Y_train)))\r\n",
    "print(\"Test score: {:.4f}\".format(pca_RForest.score(X_test_reduced, Y_test)))\r\n",
    "\r\n",
    "pca_RForest_Ypredict = pca_RForest.predict(X_test_reduced)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test, pca_RForest_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best parameters: {'max_depth': 2, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 200, 'random_state': 0}\n",
      "Best cross-validation score: 0.97\n",
      "Train score: 0.6029\n",
      "Test score: 0.5983\n",
      "Recall :0.9737\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters for the model after Grid Search with cross-validation is max_depth: 2, max_features: 2, min_samples_leaf: 5, n_estimators: 200. With these parameters, we get a test score of 0.60 and a recall score of 0.97. This is the best recall score of all the models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DEEP LEARNING"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly I am building a Deep Learning model. For the model, I use Sequential() and add two hidden layers. I use Grid search on Keras Classifier to find the best parameters- batch size and epochs. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "source": [
    "from tensorflow.keras import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "import numpy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
    "def create_model():\r\n",
    "    #create model\r\n",
    "    model = Sequential()\r\n",
    "    model.add(Dense(20, input_dim=36, activation='relu'))\r\n",
    "    model.add(Dense(12, activation='relu'))\r\n",
    "    model.add(Dense(5, activation='relu'))\r\n",
    "    model.add(Dense(1, activation='sigmoid'))\r\n",
    "    #compile model\r\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['Recall'])\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "source": [
    "seed = 10\r\n",
    "from tensorflow.random import set_seed\r\n",
    "np.random.seed(10)\r\n",
    "set_seed(10)\r\n",
    "\r\n",
    "model_grid = KerasClassifier(build_fn = create_model, verbose = 0)\r\n",
    "\r\n",
    "param_grid = {'batch_size':[10,20,30,40] , 'epochs':[10, 50, 100]}\r\n",
    "grid_search = GridSearchCV(estimator= model_grid, param_grid = param_grid, cv = 5, scoring='recall')\r\n",
    "\r\n",
    "grid_search_result = grid_search.fit(X_train, Y_train)\r\n",
    "\r\n",
    "print(grid_search.best_params_)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'batch_size': 40, 'epochs': 10}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "source": [
    "#fix random seed for reproducibility\r\n",
    "numpy.random.seed(10)\r\n",
    "\r\n",
    "set_seed(10)\r\n",
    "\r\n",
    "# create model\r\n",
    "model = Sequential()\r\n",
    "model.add(Dense(20, input_dim=36, activation='relu'))\r\n",
    "model.add(Dense(12, activation='relu'))\r\n",
    "model.add(Dense(5, activation='relu'))\r\n",
    "model.add(Dense(1, activation='sigmoid'))\r\n",
    "# Compile model\r\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['Recall', 'accuracy'])\r\n",
    "model.fit(X_train, Y_train, epochs= 10, batch_size=10, verbose=0)\r\n",
    "scores = model.evaluate(X_test, Y_test)\r\n",
    "dl_y_predict = model.predict(X_test)\r\n",
    "\r\n",
    "print('Recall Score : {}'.format(scores[1]))\r\n",
    "print('Accuracy  Score : {}'.format(scores[2]))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19/19 [==============================] - 0s 937us/step - loss: 0.6059 - recall: 0.7632 - accuracy: 0.6756\n",
      "Recall Score : 0.7631579041481018\n",
      "Accuracy  Score : 0.6756302714347839\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best parameters from the grid search are batch_size: 40, epochs: 10. When we run the Deep Learning model with these parameters, we get Accuracy = 0.68 and a Recall score =  0.76 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Summarizing the Best 10 models : <br>\n",
    "\n",
    "SVC Kernel RBF : Recall - 0.88  Accuracy - 0.64 <br>\n",
    "SVC Kernel Poly : Recall - 0.84 Accuracy - 0.65 <br>\n",
    "**Random Forest : Recall - 0.88 Accuracy - 0.70**<br>\n",
    "\n",
    "Voting Classifier (Hard) : Recall - 0.81, Accuracy - 0.66 <br>\n",
    "**Bagging With Decision Tree : Recall - 0.93 Accuracy - 0.62**<br>\n",
    "Bagging With KNN : Recall - 0.927 Accuracy - 0.63 <br> \n",
    "Pasting With Decision Tree : Recall - 0.90 Accuracy - 0.61 <br>\n",
    "Pasting With KNN : Recall - 0.90 Accuracy - 0.65 <br>\n",
    "\n",
    "PCA - Decision Tree : Recall - 0.84 Accuracy - 0.60 <br>\n",
    "**PCA - Random Forest : Recall - 0.97 Accuracy - 0.60**<br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Best model \r\n",
    "Explain which machine learning model is the best model for this dataset and why? "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From all the above models, Three models particularly standout. <br> Random Forest has a high Recall of 88% and the highest accuracy of 70%. \r\n",
    "Bagging with Decision Tree has a higher recall of 93% with a high accuracy of 62% <br>\r\n",
    "Random Forest with PCA applied reduced dataset gives the Highest Recall of 97% and a good accuracy of 60%.<br> \r\n",
    "\r\n",
    "While Random Forest and Bagging With Decision tree both have Good Recall and Accuracy combinations, ***Random Forest With PCA*** has the best Recall which is the metric of our interest. While the Accuracy is only slightly better than naive rule, the cost of wrongful predition of \"no\"s is not significant. Hence I am choosing Random Forest with PCA applied on it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "source": [
    "BestModel_RF_PCA = RandomForestClassifier(max_depth= 2, max_features= 2, min_samples_leaf= 5, n_estimators= 200, random_state=0)\r\n",
    "BestModel_RF_PCA.fit(X_train_reduced, Y_train)\r\n",
    "\r\n",
    "print(\"Train score: {:.4f}\".format(BestModel_RF_PCA.score(X_train_reduced, Y_train)))\r\n",
    "print(\"Test score: {:.4f}\".format(BestModel_RF_PCA.score(X_test_reduced, Y_test)))\r\n",
    "pca_RForest_Ypredict = BestModel_RF_PCA.predict(X_test_reduced)\r\n",
    "print('Recall :{:.4f}'.format(recall_score(Y_test, pca_RForest_Ypredict)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train score: 0.6029\n",
      "Test score: 0.5983\n",
      "Recall :0.9737\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I will now train the Best model with the entire train dataset (from train.csv) and predict the outcome of the test data set from test.csv. For this we have to use the MinMax Scaler to scale both train and test datasets and perform PCA to reduce the dimensions of the datasets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "source": [
    "#final_test_prediction\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "\r\n",
    "scaler_full = MinMaxScaler()\r\n",
    "train_full_X = scaler_full.fit_transform(X)\r\n",
    "test_full_X =  scaler_full.transform(test)\r\n",
    "\r\n",
    "pca = PCA(n_components=0.95)\r\n",
    "train_X_reduced = pca.fit_transform(train_full_X)\r\n",
    "test_X_reduced = pca.transform(test_full_X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "source": [
    "BestModel = RandomForestClassifier(max_depth= 2, max_features= 2, min_samples_leaf= 5, n_estimators= 200, random_state=0)\r\n",
    "BestModel.fit(train_X_reduced,Y)\r\n",
    "trainPredict = BestModel.predict(train_X_reduced)\r\n",
    "print(\"Recall Train score: {:.4f}\".format(recall_score(Y,trainPredict)))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Recall Train score: 0.9902\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, a prediction is made on the test data set using the best model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "source": [
    "final_test_prediction = BestModel.predict(test_X_reduced)\r\n",
    "final_test_prediction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1])"
      ]
     },
     "metadata": {},
     "execution_count": 448
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "057734934c37b6046e29060fbd45296387f602e39f8283d7c00d9ff715b1d75a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python396jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}